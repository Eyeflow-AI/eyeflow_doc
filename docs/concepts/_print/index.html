<!doctype html>
<html lang="pt" class="no-js">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.82.0" /><link rel="canonical" type="text/html" href="/docs/concepts/">
<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">


<link rel="shortcut icon" href="/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/android-36x36.png" sizes="36x36">
<link rel="icon" type="image/png" href="/favicons/android-48x48.png" sizes="48x48">
<link rel="icon" type="image/png" href="/favicons/android-72x72.png" sizes="72x72">
<link rel="icon" type="image/png" href="/favicons/android-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/favicons/android-144x144.png" sizes="144x144">
<link rel="icon" type="image/png" href="/favicons/android-192x192.png" sizes="192x192">

<title>Conceitos | Eyeflow.AI Docs</title><meta property="og:title" content="Conceitos" />
<meta property="og:description" content="Principais conceitos do Eyeflow e Vídeo Analytics
" />
<meta property="og:type" content="website" />
<meta property="og:url" content="/docs/concepts/" /><meta property="og:site_name" content="Eyeflow.AI Docs" />

<meta itemprop="name" content="Conceitos">
<meta itemprop="description" content="Principais conceitos do Eyeflow e Vídeo Analytics
"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Conceitos"/>
<meta name="twitter:description" content="Principais conceitos do Eyeflow e Vídeo Analytics
"/>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-00000000-0', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>






<link rel="preload" href="/scss/main.min.ce3463a1ef5c917875e1d7fba7e07a173525e77a633bf30508294531f61e7dd6.css" as="style">
<link href="/scss/main.min.ce3463a1ef5c917875e1d7fba7e07a173525e77a633bf30508294531f61e7dd6.css" rel="stylesheet" integrity="">


<script
  src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
  crossorigin="anonymous"></script>




  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
        <a class="navbar-brand" href="/">
		<span class="navbar-logo"><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg></span><span class="text-uppercase font-weight-bold">Eyeflow.AI Docs</span>
	</a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				<a  class="nav-link active" href="/docs/" ><span class="active">Documentação</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				<a  class="nav-link" href="/blog/" ><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				<a  class="nav-link" href="/community/" ><span>Communidade</span></a>
			</li>
			
			
			
			<li class="nav-item dropdown d-none d-lg-block">
				

<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	Português
</a>
<div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/en/docs/concepts/">English</a>
	
</div>
			</li>
			
		</ul>
	</div>
	<div class="navbar-nav d-none d-lg-block">
<input type="search" class="form-control td-search-input" placeholder="&#xf002 Buscar no site…" aria-label="Buscar no site…" autocomplete="off">

</div>
</nav>

    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
          </div>
          <div class="d-none d-xl-block col-xl-2 td-toc d-print-none">
          </div>
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
Esta é a apresentação imprimível multi-página desta seção.
<a href="#" onclick="print();return false;">Chique aqui para imprimir</a>.
</p><p>
<a href="/docs/concepts/">Retornar para a apresentação regular desta página</a>.
</p>
</div>



<h1 class="title">Conceitos</h1>
<div class="lead">Principais conceitos do Eyeflow e Vídeo Analytics</div>




    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-c5d74578d83b9f2882ac6e6033941c0f">Visão Geral</a></li>


    
  
    
    
	
<li>2: <a href="#pg-7d473224ea4bfc39a535883237df5ec8">Flow</a></li>


    
  
    
    
	
<li>3: <a href="#pg-b6a5d2943af2cc53e0d47e249ac80d37">Dataset</a></li>


    
  
    
    
	
<li>4: <a href="#pg-c4d184857c60a3b1e4c1cc383ba30ff0">Treinamento</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>4.1: <a href="#pg-821691bbab1c2ab21c91a76358a6c1c7">Visão Geral</a></li>


    
  
    
    
	
<li>4.2: <a href="#pg-cc7704bf22b8c46dbb695f0b4ca5d35f">Dashboard de Treinamento</a></li>


    
  
    
    
	
<li>4.3: <a href="#pg-c31b22b7edbbfe49e4d16d2ccdeaba6b">Parâmetros do Treinamento</a></li>


    
  
    
    
	
<li>4.4: <a href="#pg-12538f171a6b845bb3b653e76783b790">Parâmetros das Redes Neurais</a></li>


    
  
    
    
	
<li>4.5: <a href="#pg-2869d0ba31e7658c3d9261182907e451">Data Augmentation</a></li>


    
  

    </ul>
    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-c5d74578d83b9f2882ac6e6033941c0f">1 - Visão Geral</h1>
    <div class="lead">Principais conceitos do Eyeflow.AI</div>
	<h2 id="aplicação">Aplicação</h2>
<p>Conjunto de elementos que implementam um processo completo de Video Analytics.</p>
<p>Quando a gente se depara com um problema que envolve visão começamos a pensar em como podemos resolvê-lo
utilizando o Eyeflow. Na entrada temos a captura de imagens por uma câmera industrial ou um celular, e na saída
queremos ter essas imagens analisadas e interpretadas, para que ações possam ser tomadas em resposta.
Essa solução do problema é que chamamos de Aplicação.</p>
<h2 id="flow">Flow</h2>
<p>Conjunto de componentes encadeados que implementam uma aplicação.</p>
<p>No processo de construir uma aplicação o objetivo é transformar uma entrada de vídeo (<em>dados não estruturados</em>)
em uma ação programada (<em>dados estruturados</em>). Nesse processo iremos usar redes neurais e outros componentes de
software encadeados para poder decompor e interpretar as imagens de entrada e gerar as saídas desejadas.</p>
<h2 id="rede-neural">Rede Neural</h2>
<p>Algorítmo matemático avançado que permite a computação de dados não estruturados.</p>
<p>As redes neurais são elementos computacionais que vêm sendo desenvolvidos desde os anos 60, inspiradas
nas pesquisas científicas sobre o funcionamento do cérebro (<em>neurônios</em>).</p>
<p>A partir de 2010, com avanços nos
algoritmos de processamento matemático e aproximação de funções algébricas, juntamente com o aumento de
capacidade dos processadores matemáticos e barateamento pela popularização dos mesmos nas placas gráficas
para jogos (<em>GPU</em>), as redes neurais evoluíram para utilização de várias camadas (<em>Deep Neural Network</em>).
Dessa forma começaram a surgir várias novas aplicações dessa tecnologia na solução de problemas complexos,
em sua maioria relacionados a dados não estruturados, como sons e imagens.
As redes neurais são os componentes fundamentais do Flow, pois permitem a solução de problemas de visão
computacional complexos, que antes não eram passíveis de serem resolvidos utilizando algorítmos tradicionais.</p>
<p>As redes neurais possuem uma capacidade de convergir matematicamente para um modelo computacional que
identifica os padrões nos dados. Essa capacidade permitiu o surgimento de soluções de Inteligência Artificial
que aprendem com exemplos que o usuário entrega para a rede.</p>
<h2 id="dataset">Dataset</h2>
<p>Conjunto de exemplos que instruem o aprendizado da rede neural.</p>
<p>Uma rede neural é como uma caixa preta onde vamos inserindo exemplos na entrada e vamos dizendo o que queremos
ter na saída. Assim, o usuário deve gerar um conjunto desses exemplos, anotados com a saída desejada, que servirão
de dados para treinamento da rede neural na execução da tarefa.</p>
<h2 id="treinamento">Treinamento</h2>
<p>Processo computacional para convergir uma rede neural para o aprendizado dos padrões.</p>
<p>Depois que temos um dataset anotado colocamos os algorítmos de redes neurais para processar esses exemplos
até ela aprender a gerar a saída desejada. O treinamento busca reduzir o erro medido entre o que a rede neural
apresentou na saída em relação à anotação do exemplo feita pelo usuário. Quando esse erro é bem reduzido a
rede neural estará pronta para processar novos dados.</p>
<h2 id="modelo">Modelo</h2>
<p>Rede neural treinada com um dataset.</p>
<p>Após o processo de treinamento é gerado um modelo, que poderá então ser utilizado no Flow para processamento
das imagens e geração das saídas desejadas.</p>
<h2 id="edge--borda">Edge / Borda</h2>
<p>Dispositivo computacional que executa um flow em produção.</p>
<p>Depois que um flow está desenvolvido, testado e apresentando bons resultados ele pode ser publicado para
execução em um dispositivo que passará a funcionar em produção, como por exemplo a detecção de um defeito
na linha de manufatura.</p>
<h2 id="para-onde-devo-ir-agora">Para onde devo ir agora?</h2>
<ul>
<li><a href="/docs/Concepts/dataset/">Dataset</a>: Anotando Datasets</li>
<li><a href="/docs/Concepts/flow/">Flow</a>: Criando o Flow</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7d473224ea4bfc39a535883237df5ec8">2 - Flow</h1>
    <div class="lead">Diagrama de fluxos para processamento das imagens e tomadas de decisões</div>
	<p>O Flow é um programa no formato low-code, construído com componentes de código, orientado para a decomposição de dados de imagens
(<em>não estruturados</em>) em dados de saída (<em>estruturados</em>).</p>
<p><strong>O Flow é estruturado como um processo de entrada-saída.</strong></p>
<p>A entrada é um fluxo de imagens, como por exemplo:</p>
<ul>
<li>Câmera: Industrial, Segurança, IP</li>
<li>Celular</li>
<li>Arquivo de Vídeo</li>
</ul>
<p>A saída pode ser para qualquer outro sistema de dados como:</p>
<ul>
<li>Arquivos: JSON, CSV, TXT</li>
<li>Bancos de dados: MongoDB, Postgres</li>
<li>Filas de mensagem: RabbitMQ, AMQP</li>
<li>Controladores: CLP, IC, TCP, RS-232</li>
<li>Envio de alarmes: E-mail, SMS, Notificações</li>
</ul>
<p>Como o Eyeflow é uma plataforma extensível, não existe limites para criação de componentes. É possível criar rapidamente um componente
em Python e usá-lo no Flow para executar a função desejada.</p>
<p>Entre a entrada e a saída o processamento das imagens é feito em componentes de Redes Neurais. Cada componente desses estará ligado a
um dataset, e irá usar o modelo treinado com o dataset para executar em produção.</p>
<p>É possível também utilizar componentes que enviam as imagens para processamento em serviços de provedores de cloud. Por exemplo, usar um
componente que pega a imagem de um documento e envia para o reconhecimento de caracteres do Google, e daí processar a resposta para
digitalizar esse documento.</p>
<p>Criar um Flow é um processo bem simples.</p>
<h4 id="clique-na-aba-flow-na-barra-de-navegação">Clique na aba Flow na barra de navegação</h4>
<p>Abrirá a tela para carregar um Flow ou Criar um novo.
Ao clicar em <strong>Novo Flow</strong> irá abrir uma tela para entrar com os dados. É só preencher os dados e pronto, o Flow será criado.</p>
<p><img src="/screenshots/pt-br_create_flow.jpg#bordered" alt="Criar Flow" title="Criar Flow"></p>
<p>Após a criação do Flow vamos adicionar alguns componentes para criar um flow simples que identifica de o Pet na imagem.</p>
<h3 id="vamos-adicionar-3-componentes-nesse-flow">Vamos adicionar 3 componentes nesse Flow:</h3>
<h4 id="input-câmera-ip---coloque-localhost-como-url">Input Câmera IP - Coloque <code>localhost</code> como URL</h4>
<p><img src="/screenshots/pt-br_flow_camera_ip.jpg#bordered" alt="Componente Camera IP" title="Componente Camera IP"></p>
<h4 id="roi-cutter---coloque-o-nome-identifica-pet-e-escolha-o-dataset-gatos-e-cachorros">ROI Cutter - Coloque o <code>nome</code> Identifica Pet e escolha o dataset <code>Gatos e Cachorros</code></h4>
<p><img src="/screenshots/pt-br_flow_roi_cutter.jpg#bordered" alt="Componente ROI Cutter" title="Componente ROI Cutter"></p>
<h4 id="output-json-file-save---preencha-filename-como-testjson">Output JSON File Save - Preencha <code>filename</code> como <code>test.json</code></h4>
<p><img src="/screenshots/pt-br_flow_file_save.jpg#bordered" alt="Componente File Save" title="Componente File Save"></p>
<h4 id="ligue-as-saídas-de-cada-componente-na-entrada-do-próximo">Ligue as saídas de cada componente na entrada do próximo</h4>
<p><img src="/screenshots/pt-br_flow_basic.jpg#bordered" alt="Flow Completo" title="Flow Completo"></p>
<p><strong>Pronto! O Flow está completo.</strong></p>
<p>Uma das grandes vantagens da plataforma EyeFlow.AI é que ela é um ambiente integrado completo para o desenvolvimento de aplicações
de Video Analytics. O processo de redes neurais exige uma razoável quantidade de exemplos anotados para poder aprender, e a
maneira mais fácil de conseguir esses exemplos é utilizando vídeos.</p>
<p>Então, o processo de desenvolvimento envolve um ciclo de atividades:</p>
<ol>
<li>Testa o vídeo no Flow observando se a aplicação está marcando o vídeo corretamente</li>
<li>Identifica os erros que a aplicação está cometendo e vai nos datasets</li>
<li>Na tela de <strong>Novos Exemplos</strong> procura por quadros onde a rede neural não identificou o objeto corretamente</li>
<li>Adiciona vários exemplos de erro (de 30 a 50 em cada ciclo é um bom número)</li>
<li>Anota os novos exemplos corrigindo os erros</li>
<li>Treina a rede neural e verifica os indicadores de assertividade para ver se a rede está aprendendo bem</li>
<li>Após os treinamentos volta para testar novamente o vídeo</li>
</ol>
<p>Após termos bons resultados nas anotações dos vídeos nosso Flow já está pronto para publicarmos na borda.</p>
<h2 id="para-onde-devo-ir-agora">Para onde devo ir agora?</h2>
<ul>
<li><a href="/docs/concepts/dataset/">Dataset</a>: Anotando Datasets</li>
<li><a href="/docs/concepts/training/">Treinamento</a>: Treinando a rede neural</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b6a5d2943af2cc53e0d47e249ac80d37">3 - Dataset</h1>
    <div class="lead">Conjunto de exemplos anotados para treinamento das redes neurais</div>
	<!-- <blockquote class="note callout">
    <div><strong>Observação:</strong> </div>
</blockquote> -->
<p>Um dataset é um conjunto de exemplos anotados que servem para instruir uma rede neural sobre o que é desejado
que seja reconhecido nas imagens.
No conjunto de imagens do dataset iremos ter vários exemplos diferentes dos objetos que desejamos identificar
ou reconhecer. Esses exemplos irão alimentar o treinamento da rede neural para que possa ser identificado o padrão
comum entre eles.</p>
<h2 id="tipos-de-dataset">Tipos de dataset</h2>
<p>Os datasets podem ser de vários tipos. Os mais utilizados no Eyeflow são:</p>
<ul>
<li><a href="#objectdetection">ObjectDetection</a></li>
<li><a href="#classification">Classification</a></li>
<li><a href="#anomallydetection">AnomallyDetection</a></li>
<li><a href="#instancesegmentation">InstanceSegmentation</a></li>
</ul>
<p>Cada tipo de dataset possui uma anotação diferente, e um diferente tipo de saída.</p>
<h3 id="objectdetection">ObjectDetection</h3>
<p>Os datasets ObjectDetection costumam ser os mais utilizados pela sua versatilidade e facilidade de anotação.
O objetivo é identificar um objeto específico em uma imagem ampla, delimitando uma área retangular ao redor do objeto.
Sua anotação costuma ser bem simples e rápida, pois basta clicar e arrastar um retângulo (<em>box</em>) e definir qual a
classe do objeto.
É executado por um tipo de rede neural que pode ser bastante otimizada para execução na borda, podendo chegar a
centenas de FPS em tempo real.</p>
<h3 id="classification">Classification</h3>
<p>Os datasets de Classification são os mais comuns, e foram os primeiros a surgir no universo de DeepLearning.
Basicamente identifica uma imagem como sendo uma de N classes. Por exemplo, se tiver uma lista de imagens de gato
pode-se definir qual a raça de cada um, e a rede neural irá aprender a identificar a raça em qualquer imagem.</p>
<h3 id="anomallydetection">AnomallyDetection</h3>
<blockquote class="note callout">
    <div><strong>Observação:</strong> Este é um dataset ainda em fase experimental.</div>
</blockquote>
<p>O dataset AnomallyDetection é orientado para identificar anomalias em imagens de um mesmo objeto. A ideia é
alimentar o dataset com muitas imagens do objeto &ldquo;correto&rdquo; e assim a rede neural aprender o padrão bom.
Quando a rede neural for apresentada a uma imagem de um objeto que tem uma diferença do padrão bom (anomalia),
ela irá apontar a anomalia.</p>
<h3 id="instancesegmentation">InstanceSegmentation</h3>
<blockquote class="note callout">
    <div><strong>Observação:</strong> Este é um dataset ainda em fase experimental.</div>
</blockquote>
<p>InstanceSegmentation é um dataset que se popularizou nos últimos anos devido a sua intensa utilização nos sistemas
de veículos autônomos e sistemas de IA para medicina. Trata-de de identificar o objeto, juntamente com todo seu contorno
pixel a pixel. Sua saída é bastante impressionante, pois permite &ldquo;recortar&rdquo; detalhadamente o objeto do fundo da imagem,
porém, sua anotação é extremamente trabalhosa, geralmente demandando uma grande equipe de pessoas para gerar um
conjunto adequado de imagens anotadas para treinamento.</p>
<h2 id="criando-um-dataset">Criando um dataset</h2>
<h4 id="criar-dataset-é-uma-tarefa-bem-simples-basta-clicar-no-menu-lateral-em-novo-dataset">Criar dataset é uma tarefa bem simples. Basta clicar no menu lateral em <strong>Novo Dataset</strong>.</h4>
<p><img src="/screenshots/pt-br_create_dataset.jpg#bordered" alt="Criar Dataset" title="Criar Dataset"></p>
<h4 id="irá-abrir-uma-tela-para-entrar-com-os-dados-do-dataset">Irá abrir uma tela para entrar com os dados do dataset.</h4>
<p><img src="/screenshots/pt-br_create_dataset_modal.jpg#bordered" alt="Criar Dataset" title="Criar Dataset"></p>
<p>É só preencher o Nome e Descrição, escolher o tipo e definir a qual Aplicação o dataset pertence.
Depois é preciso adicionar pelo menos uma classe.</p>
<h2 id="classes">Classes</h2>
<p>O objetivo da rede neural será identificar um padrão na imagem, e dar saída disso em formato de dados. Assim, criamos
classes no dataset para poder usarmos para marcar as imagens, e assim ensinar a rede neural a reconhecer esses padrões
e nos informar qual foi a classe reconhecida.
Por exemplo, se temos várias imagens de exemplos de cachorros e gatos, e queremos saber se trata-se de um ou outro na
imagem, criamos duas classes:</p>
<p><img src="/screenshots/pt-br_create_classes_modal.jpg#bordered" alt="Criar Classes" title="Criar Classes"></p>
<p>As cores escolhidas servirão para ajudar na anotação do dataset, e irão aparecer na anotação dos vídeos.</p>
<h2 id="exemplos">Exemplos</h2>
<p>Exemplos são imagens anotadas que irão instruir a rede neural no aprendizado do reconhecimento dos padrões. Inserimos
várias imagens no dataset a partir de quadros de um video ou fotos, e depois anotamos em cada uma dessas imagens
aquilo que é desejado ser reconhecido.
A maneira mais simples e rápida de adicionar novos exemplos é extraindo de vídeos de exemplo. Ao passar um vídeo em
um Flow a ferramenta extrai alguns dos quadros do vídeo e os disponibiliza na tela de Novos Exemplos.</p>
<p><img src="/screenshots/pt-br_menu_new_examples.jpg#bordered" alt="Menu Novos Exemplos" title="Menu Novos Exemplos"></p>
<p>Nessa tela também é possivel subir novos exemplos a partir de imagens localizadas no computador.</p>
<p><img src="/screenshots/pt-br_insert_new_examples.jpg#bordered" alt="Inserir Novos Exemplos" title="Inserir Novos Exemplos"></p>
<blockquote class="note callout">
    <div> <h5 style="color:mediumblue"><i class="fas fa-lightbulb"></i>&nbspImportante!</h5> Para que uma rede neural aprenda bem é importante que haja uma boa diversidade
de exemplos. Inserir várias imagens muito parecidas, ou inserir muitas imagens de uma classe e poucas das outras
irá fazer com que o dataset fique desbalanceado e a rede não aprende bem</div>
</blockquote>
<h2 id="anotação">Anotação</h2>
<p>Anotação é o processo onde o usuário marca na imagem o objeto que deseja que seja identificado, gerando assim um exemplo
que será utilizado no treinamento da rede neural.</p>
<p>As anotações são diferentes para cada tipo de dataset. Em um dataset <a href="#objectdetection">ObjectDetection</a> o usuário clica e
arrasta o mouse para desenhar uma caixa ao redor do objeto, e depois define a qual classe esse objeto pertence.</p>
<p><img src="/screenshots/pt-br_annotate_example.jpg#bordered" alt="Anotar Exemplo" title="Anotar Exemplo"></p>
<h2 id="para-onde-devo-ir-agora">Para onde devo ir agora?</h2>
<ul>
<li><a href="/docs/concepts/training/">Treinamento</a>: Treinando a rede neural</li>
<li><a href="/docs/concepts/flow/">Flow</a>: Criando o Flow</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c4d184857c60a3b1e4c1cc383ba30ff0">4 - Treinamento</h1>
    <div class="lead">Processo de aprendizado das redes neurais</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-821691bbab1c2ab21c91a76358a6c1c7">4.1 - Visão Geral</h1>
    <div class="lead">Visão geral do treinamento</div>
	<p>Treinamento é o processo onde os exemplos do dataset são alimentados na rede neural para aprendizado dos
padrões e geração do modelo.
É um processo matemático/algorítmico complexo, que exige uma grande capacidade computacional e necessita
rodar em servidores com placas gráficas GPU ou TPU no caso da cloud Google.</p>
<p>Esse processo é executado de forma iterativa buscando minimizar o erro medido entre o que a rede neural
apresentou na saída e a anotação feita no exemplo pelo usuário.
Ao longo do processo de minimização do erro é medida a assertividade da rede neural, e assim o usuário
pode verificar se a rede neural realmente aprendeu a identificar os padrões.</p>
<p>O processo de treinamento das redes neurais é bastante complexo, e dominar bem esse processo costuma demandar
bastante tempo, estudo e experimentação.</p>
<p>Felizmente, o Eyeflow.AI oferece um ambiente completo para acelerar e automatizar todas as tarefas para o desenvolvimento
de redes neurais, e possui sugestões de construção e parametrização, fruto de longos anos de experiência de uso
de redes neurais para solução de problemas reais.</p>
<h2 id="conceitos-importantes">Conceitos importantes</h2>
<h3 id="quantidade-de-exemplos">Quantidade de exemplos</h3>
<p>A quantidade de exemplos de um dataset é um item muito importante para a eficácia das detecções. Para realizar
um treinamento de uma rede neural é necessário no mínimo 10 exemplos, sendo que um bom dataset para produção
vai demandar cerca de 1000 exemplos. Contudo, não adianta simplesmente lotar um dataset de exemplos sem testar
sua eficácia com casos reais, pois um excesso de exemplos semelhantes pode desbalancear o aprendizado e fazer com que seu
modelo fique enviesado e não tenha boa performance em produção.</p>
<p>Outro ponto importante é que deve-se manter uma quantidade similar de exemplos de cada classe, também para evitar
o desbalanceamento, e consequentemente enviesamento, do modelo. Se o dataset tiver muitos exemplos de uma só
classe e poucos das outras ele irá tender a detectar tudo como a classe dominante.</p>
<p>A melhor estratégia para adição de novos exemplos no dataset é testar o mesmo após o treinamento e verificar
os erros, para adicionar somente os novos exemplos que apresentaram erro, e daí corrigí-los e fazer um novo treinamento.
A cada iteração dessas o ideal é adicionar de 30 a 50 novos exemplos, e daí rapidamente o treinamento irá aumentar
a assertividade da detecção.</p>
<p>Após os testes com vídeos e publicação em produção o Eyeflow oferece um mecanismo para coleta de novos exemplos a
partir da borda, o que servirá para verificar se o modelo está tendo uma boa assertividade nos casos reais, e daí
podem ser adicionados os erros para continuar o processo de melhoria do dataset.</p>
<h3 id="qualidade-dos-exemplos">Qualidade dos exemplos</h3>
<p>Se a quantidade é importante, a qualidade é fundamental para garantir que a rede neural irá aprender a detectar os
padrões.</p>
<p>Para garantir a qualidade dos exemplos alguns pontos devem ser observados:</p>
<ul>
<li>O objeto a ser detectado deve estar bem visível na imagem. Se houver dúvidas para anotar o exemplo então a própria
rede neural terá dificuldades em aprender com esse exemplo.</li>
<li>Para um dataset do tipo ObjectDetection as caixas não podem ser muito pequenas, nem grandes demais. Os testes de augmentation
vão indicar se as caixas estão sendo bem detectadas.</li>
<li>Para um dataset Classification os exemplos precisam ser bem diferentes entre as classes. Será difícil detectar uma diferença
entre as classes se tal diferença for somente um pequeno detalhe na imagem total.</li>
<li>Deve-se ter uma boa variedade nos exemplos. A adição de muitos exemplos muito parecidos irá fazer o modelo ficar enviesado
para detectar só esse tipo de imagem ou classe.</li>
</ul>
<h3 id="validação-e-teste">Validação e Teste</h3>
<p>No processo de treinamento o dataset é dividido em 3 grupos:</p>
<ul>
<li>Treino</li>
<li>Validação</li>
<li>Teste</li>
</ul>
<p>A separação dos exemplos para os grupos é feita de forma aleatória, baseada nos parâmetros definidos para o treinamento.</p>
<p>O grupo de <strong>Treino</strong> é o que será efetivamente usado para treinar a rede neural. Ele deve ter a maior quantidade de exemplos,
sendo recomendado pelo menos 80% dos exemplos.</p>
<p>O grupo de <strong>Validação</strong> é usado para testar o modelo a cada final de época, e serve para verificar se o treino está evoluindo,
ou se estagnou ou viciou (<em>overfitting</em>). É recomendado que seja 10% dos exemplos até um máximo de 100 exemplos.</p>
<p>O grupo de <strong>Teste</strong> é usado para testar o modelo ao final do treinamento. Como esses exemplos nunca foram apresentados ao
dataset então servem como uma avaliação final do aprendizado, definindo qual a assertividade que o modelo apresenta. É o indicador
se a rede neural efetivamente aprendeu a reconhecer os padrões. É recomendado que seja 10% dos exemplos até um máximo de 100 exemplos.</p>
<blockquote class="note callout">
    <div><strong>Observação:</strong> O principal objetivo de um treinamento de rede neural é garantir que o modelo final irá <strong>generalizar</strong>, significa que irá
aprender com os exemplos o padrão para assim poder extrapolar para novos exemplos em situações reais. Muitas vezes o resultado
fica bom no treino, mas ruim em produção, indicando que o modelo não generalizou bem. Nessas situações deve-se coletar mais
exemplos de erro em produção para adicionar ao dataset.</div>
</blockquote>
<h3 id="loss-e-val-loss">Loss e Val Loss</h3>
<p>Loss é a medida de erro que no processo de treinamento foi observado entre o que a rede neural detectou (<em>predict</em>) e o que deveria
ser a saída correta segundo a anotação (<em>ground thruth</em>). O processo matemático/algorítmico de treinamento das redes neurais busca
a minimização desse erro.</p>
<p>O Val Loss é a mesma medida, só que feita no grupo de validação ao final de uma época. Serve para acompanhar a evolução do treino
a cada ciclo de treinamento.</p>
<h3 id="map-e-accuracy">mAP e Accuracy</h3>
<p>O indicador de qualidade do treino é uma medida de acerto que leva em consideração todos os elementos de saída. Cada tipo de
dataset tem sua medida padrão de assertividade.</p>
<ul>
<li>Datasets <em>ObjectDetection</em> costumam utilizar o <strong>mAP</strong> (<em>Mean Average Precision</em>) como indicador de assertividade, pois mede o quanto a
box de saída engloba bem o objeto, indicando a classe correta, pela média de todas as classes.</li>
<li>Datasets <em>Classification</em> costumam utilizar a <strong>Accuracy</strong>, que é o percentual de exemplos em que a rede neural previu corretamente
a qual classe o objeto pertence.</li>
</ul>
<h3 id="overfitting">Overfitting</h3>
<p>É um fenômeno que ocorre comumento nos treinamentos. Ocorre no treinamento quando vemos o <em>Loss</em> continuar caindo, mas o <em>Val Loss</em> subir
ao invés de descer. Isso indica que a rede neural ficou &ldquo;viciada&rdquo; nos exemplos do treinamento e parou de generalizar para novos exemplos,
ou seja, ela aprendeu muito sobre os exemplos do treino, mas não consegue acertar exemplos novos.</p>
<h3 id="expansão-dos-exemplos---data-augmentation">Expansão dos exemplos - Data Augmentation</h3>
<p>É um trabalho demorado e repetitivo anotar os exemplos de um dataset, sendo porém a tarefa mais importante para poder usar uma
rede neural em uma aplicação real. Muitos projetos de AI estagnam na parte em que precisam reunir muitos milhares de exemplos anotados
para começar a ter bons resultados com redes neurais, mas a experiência com o Eyeflow.AI indica que é possível ter bons resultados apenas
com 300 exemplos em um dataset, e ótimos resultados com apenas 1000 exemplos.
Um dos aspectos chaves dessa performance está no Data Augmentation.</p>
<p>Data Augmentation é o processo de inserir perturbações nas imagens dos exemplos, no momento do treinamento, para forçar a rede neural
a aprender os padrões dos objetos, e não ficar &ldquo;viciada&rdquo; nos poucos exemplos que foram apresentados.</p>
<p>O Eyeflow.AI oferece uma ampla gama de algoritmos para Data Augmentation, que vão desde distorções geométricas e fotométricas nas imagens
até inserção de ruidos ou elementos estranhos.</p>
<p>O ponto importante a observar aqui é que não deve-se usar o Data Augumentation para modificar os exemplos deixando-os muito diferentes
das situação que serão encontradas no ambiente real, pois isto irá dificultar desnecessariamente o aprendizado da rede. Por exemplo, não
faria sentido em um dataset de carros inserir o <em>Flip Vertical</em>, pois não encontraremos uma situação com o carro de cabeça para baixo.</p>
<h3 id="datasets-sintéticos">Datasets sintéticos</h3>
<p>Da mesma forma que o Data Augumentation ajuda a melhorar o aprendizado da rede com poucos exemplos, um Dataset Sintético ajuda a gerar
exemplos automaticamente para o treinamento da rede.</p>
<p>São poucas situação em que dá pra usar datasets sintéticos, uma delas é para reconhecimento de caracteres (<em>OCR</em>) por exemplo, mas quando
é possível os ganhos são enormes, pois dá pra gerar um modelo extremamente robusto e eficaz.</p>
<p>O Eyeflow.AI é uma plataforma expansível, que permite ao usuário a criação de seus próprios componentes de redes neurais e geração de datasets
sintéticos.</p>
<h2 id="para-onde-devo-ir-agora">Para onde devo ir agora?</h2>
<ul>
<li><a href="/docs/concepts/training/train_parms">Parâmetros de Treinamento</a>: Parâmetros gerais para treinamento da rede neural</li>
<li><a href="/docs/concepts/training/dnn_parms">Parâmetros de Redes Neurais</a>: Parâmetros específicos da rede neural</li>
<li><a href="/docs/concepts/training/data_augmentation_parms">Parâmetros de Expansão de Dados</a>: Parâmetros para expansão de dados - Data Augmentation</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-cc7704bf22b8c46dbb695f0b4ca5d35f">4.2 - Dashboard de Treinamento</h1>
    <div class="lead">Visão geral dos treinamentos</div>
	<p>O Dashboard de Treinamentos dá uma visão geral de todo processo de treinamento. Nessa tela podemos acompanhar os
treinamentos em tempo real, bem como avaliar o histórico dos treinamentos anteriores e editar vários parâmetros.</p>
<p><img src="/screenshots/pt-br_dashboard_overview.jpg#bordered" alt="Visão Geral" title="Visão Geral"></p>
<h2 id="indicadores">Indicadores</h2>
<p>Nesses quadros estão os indicadores do treinamento selecionado. A data que foi realizado, duração, e resultados
finais do treino.
Temos também uma barra de botões com ações como <em>Download</em> e <em>Publicação</em> do modelo, visualização dos parâmetros
e visualização das imagens de teste.</p>
<p><img src="/screenshots/pt-br_dashboard_kpis.jpg#bordered" alt="Indicadores" title="Indicadores"></p>
<h3 id="resultados-finais-do-treino">Resultados finais do treino</h3>
<p><img src="/screenshots/pt-br_dashboard_final_results.jpg#bordered" alt="Resultados Finais" title="Resultados Finais"></p>
<h3 id="imagem-de-teste-de-expansão-de-exemplos---data-augmentation">Imagem de teste de expansão de exemplos - Data Augmentation</h3>
<p><img src="/screenshots/pt-br_dashboard_test_augmentation.jpg#bordered" alt="Teste de Expansão" title="Teste de Expansão"></p>
<h3 id="imagem-de-teste-do-final-do-treino">Imagem de teste do final do treino</h3>
<p><img src="/screenshots/pt-br_dashboard_test_final.jpg#bordered" alt="Teste Final" title="Teste Final"></p>
<h2 id="gráficos">Gráficos</h2>
<p>Os gráficos dão uma ideia da progressão do treino, seja em tempo real ou histórico.</p>
<p><strong>Loss</strong> e <strong>Val Loss</strong> indicam o erro medido no treino, e são indicadores comuns a todos os treinos. Devem ser curvas
descendentes que se aproximam de 0. Quanto menor o Loss menos a rede neural está errando.</p>
<p><img src="/screenshots/pt-br_dashboard_graphs.jpg#bordered" alt="Gráficos" title="Gráficos"></p>
<p>Existem também gráficos de eficácia da rede neural, e para esses quanto maior melhor.</p>
<ul>
<li><strong>mAP</strong> para datasets <em>ObjectDetection</em>. Geralmente já é considerado bom acima de 0.6.</li>
<li><strong>Accuracy</strong> para datasets de <em>Classification</em>. É considerado bom acima de 90%.</li>
</ul>
<h2 id="histórico-de-treinamentos">Histórico de Treinamentos</h2>
<p>É possível também verificar os treinamentos anteriores e assim ter insights sobre se as ações que foram tomadas na
construção do dataset e parametrização dos treinos surtiram efeitos positivos ou negativos.</p>
<p><img src="/screenshots/pt-br_dashboard_history.jpg#bordered" alt="Histórico" title="Histórico"></p>
<h2 id="para-onde-devo-ir-agora">Para onde devo ir agora?</h2>
<ul>
<li><a href="/docs/concepts/dataset/">Dataset</a>: Anotando Datasets</li>
<li><a href="/docs/concepts/training/">Treinamento</a>: Treinando a rede neural</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c31b22b7edbbfe49e4d16d2ccdeaba6b">4.3 - Parâmetros do Treinamento</h1>
    <div class="lead">Controle do treinamento</div>
	<h2 id="iterações">Iterações</h2>
<p>O processo de treinamento é iterativo, ou seja, realizado em ciclos. Cada ciclo é chamado de <strong>Época</strong>.
Em cada época todos os exemplos são apresentados para a rede neural, para que sejam aprendidos os padrões. O
algoritmo de treinamento vai então medindo o erro (<em>Loss</em>) e ajustando a rede neural para ir minizando o mesmo.</p>
<p>É comum se pensar de que com mais épocas a rede irá aprender mais, mas na prática não é exatamente assim. De acordo
com a quantidade/qualidade dos exemplos chega-se a um ponto em que o erro não diminui mais e o aprendizado estagna. Outra
ocorrência comum é o erro (<em>Loss</em>) continuar diminuindo, mas o <em>Val Loss</em> começar a aumentar. Esse fenômeno é conhecido
como <em>Overfitting</em> e significa que a rede neural ficou viciada nos exemplos do treinamento e não está mais conseguindo
generalizar para novos exemplos.</p>
<p>A recomendação é no início setar 5 épocas para o treino enquanto o dataset tiver menos de 100 exemplos, e daí ir subindo.</p>
<p>Na sequência temos vários outros parâmetros que governam o processo de treinamento e todos podem influir positiva ou negativamente
no resultado final do aprendizado da rede. Não se amedronte com a quantidade, nem com a complexidade dos mesmos, é natural
levar bastante tempo até adquirir domínio sobre todo proceso.</p>
<p>Fique tranquilo, o Eyeflow.AI tem um ótimo set de defaults para os parâmetros que resolvem as necessidades de boa parte das
necessidades. Além disso, nosso time está à disposição para tirar dúvidas e dar dicas em nosso Fórum.</p>
<!-- <parm_table> -->
<h2 id="parâmetros-de-treinamento">Parâmetros de treinamento</h2>
<p><strong>Parâmetros para treinamento de rede neural</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Épocas</td>
<td>int [1 - 200]</td>
<td>5</td>
<td>Número de épocas para treinamento</td>
</tr>
<tr>
<td>Etapas por época</td>
<td>int [50 - 2000]</td>
<td>100</td>
<td>Número de passos para treinamento em cada época</td>
</tr>
<tr>
<td>Tamanho do lote</td>
<td>int [1 - 64]</td>
<td>10</td>
<td>Número de exemplos em cada passo</td>
</tr>
<tr>
<td>Tamanho do Val</td>
<td>number [0.01 - 0.9]</td>
<td>0.1</td>
<td>Porcentagem de exemplos selecionados para validação</td>
</tr>
<tr>
<td>Tamanho do Teste</td>
<td>number [0.01 - 0.9]</td>
<td>0.1</td>
<td>Porcentagem de exemplos selecionados para o teste final</td>
</tr>
<tr>
<td>Limiar de confiança</td>
<td>number [0.05 - 1.0]</td>
<td>0.6</td>
<td>Limite mínimo de confiança para detecção válida</td>
</tr>
<tr>
<td>Limite de detecção de IoU</td>
<td>number [0.05 - 1.0]</td>
<td>0.45</td>
<td>Limite mínimo para detecção de IoU</td>
</tr>
<tr>
<td>Máximo de caixas</td>
<td>int [1 - 300]</td>
<td>30</td>
<td>Número máximo de caixas em detecção</td>
</tr>
<tr>
<td>Caixas de expansão</td>
<td>number [0 - 2]</td>
<td>0</td>
<td>Porcentagem para expandir o tamanho das caixas na detecção</td>
</tr>
</tbody>
</table>
<h3 id="resolução-de-entrada">Resolução de entrada</h3>
<p><strong>Dimensões da imagem de entrada</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Lado mínimo</td>
<td>int [20 - 800]</td>
<td>50</td>
<td>Tamanho do lado menor</td>
</tr>
<tr>
<td>Lado máximo</td>
<td>int [20 - 1000]</td>
<td>80</td>
<td>Tamanho do lado maior</td>
</tr>
<tr>
<td>Canais</td>
<td>choice [1, 3]</td>
<td>1</td>
<td>Canais de cor</td>
</tr>
</tbody>
</table>
<h3 id="parâmetros-do-otimizador">Parâmetros do Otimizador</h3>
<p><strong>Parâmetros para Train Optimizer</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>beta_2</td>
<td>number [0.1 - 1.0]</td>
<td>0.999</td>
<td>Beta 2</td>
</tr>
<tr>
<td>beta_1</td>
<td>number [0.1 - 1.0]</td>
<td>0.9</td>
<td>Beta 1</td>
</tr>
<tr>
<td>Taxa de Aprendizagem</td>
<td>number [1e-06 - 0.1]</td>
<td>0.001</td>
<td>Taxa de aprendizagem do otimizador</td>
</tr>
<tr>
<td>amsgrad</td>
<td>bool [True - False]</td>
<td>False</td>
<td>amsgrad</td>
</tr>
</tbody>
</table>
<h3 id="parada-antecipada">Parada Antecipada</h3>
<p><strong>Parada antecipada para treinamento</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Paciência</td>
<td>int [1 - ]</td>
<td>5</td>
<td>Número de épocas para esperar pelo progresso</td>
</tr>
<tr>
<td>Variável de monitoramento</td>
<td>choice [&lsquo;val_loss&rsquo;, &lsquo;loss&rsquo;, &lsquo;categorical_accuracy&rsquo;, &lsquo;val_categorical_accuracy&rsquo;]</td>
<td>val_loss</td>
<td>Variável para monitorar o progresso</td>
</tr>
<tr>
<td>Delta Mínimo</td>
<td>number [0 - ]</td>
<td>0.01</td>
<td>A variação mínima na variável</td>
</tr>
<tr>
<td>Modo de avaliação</td>
<td>choice [&lsquo;min&rsquo;, &lsquo;max&rsquo;, &lsquo;auto&rsquo;]</td>
<td>min</td>
<td>Monitor do decréscimo ou incremento do valor da variável</td>
</tr>
</tbody>
</table>
<h3 id="reduzir-lr-no-platô">Reduzir LR no platô</h3>
<p><strong>Reduza a taxa de aprendizagem no platô</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Paciência</td>
<td>int [1 - ]</td>
<td>4</td>
<td>Número de épocas para esperar pelo progresso</td>
</tr>
<tr>
<td>Variável de monitoramento</td>
<td>choice [&lsquo;val_loss&rsquo;, &lsquo;loss&rsquo;, &lsquo;categorical_accuracy&rsquo;, &lsquo;val_categorical_accuracy&rsquo;]</td>
<td>val_loss</td>
<td>Variável para monitorar o progresso</td>
</tr>
<tr>
<td>Delta Mínimo</td>
<td>number [0 - ]</td>
<td>0.01</td>
<td>A variação mínima na variável</td>
</tr>
<tr>
<td>Fator de redução</td>
<td>number [0.1 - 0.9]</td>
<td>0.5</td>
<td>Quantidade a reduzir</td>
</tr>
<tr>
<td>Esfriar</td>
<td>number [0 - ]</td>
<td>0</td>
<td>Esfriar</td>
</tr>
</tbody>
</table>
<h3 id="salvar-ponto-de-verificação">Salvar ponto de verificação</h3>
<p><strong>Gatilho para salvar o progresso do treinamento do modelo</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Variável de monitoramento</td>
<td>choice [&lsquo;val_loss&rsquo;, &lsquo;loss&rsquo;, &lsquo;categorical_accuracy&rsquo;, &lsquo;val_categorical_accuracy&rsquo;]</td>
<td>val_loss</td>
<td>Variável para monitorar para salvar</td>
</tr>
<tr>
<td>Modo de avaliação</td>
<td>choice [&lsquo;min&rsquo;, &lsquo;max&rsquo;, &lsquo;auto&rsquo;]</td>
<td>min</td>
<td>Salvar ao diminuir ou aumentar o valor da variável</td>
</tr>
</tbody>
</table>
<!-- </parm_table> -->
<h2 id="para-onde-devo-ir-agora">Para onde devo ir agora?</h2>
<ul>
<li><a href="/docs/concepts/training/train_parms">Parâmetros de Treinamento</a>: Parâmetros gerais para treinamento da rede neural</li>
<li><a href="/docs/concepts/training/dnn_parms">Parâmetros de Redes Neurais</a>: Parâmetros específicos da rede neural</li>
<li><a href="/docs/concepts/training/data_augmentation_parms">Parâmetros de Expansão de Dados</a>: Parâmetros para expansão de dados - Data Augmentation</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-12538f171a6b845bb3b653e76783b790">4.4 - Parâmetros das Redes Neurais</h1>
    <div class="lead">Controle das redes neurais</div>
	<p>Nesse conjunto de parâmetros são controlados os aspectos relacionados à arquitetura das redes neurais que irão
aprender com os exemplos do dataset no treinamento.</p>
<p>Cada tipo de rede neural tem sua própria arquitetura, parâmetros peso e performance. O Eyeflow.AI é uma plataforma
extensível, que permite se trabalhar com as mais diversas arquiteturas. Entretanto, nossa experiência de vários
projetos já nos ensinou sobre várias arquiteturas que funcionam bem em produção, e é essa experiência que
buscamos trazer para a plataforma, e assim simplificar a vida dos usuários.</p>
<p>Nessa fase Beta temos 2 principais componentes que usamos para solucionar todos os problemas que temos encontrado.</p>
<!-- <parm_table> -->
<h2 id="parâmetros-de-rede-neural">Parâmetros de rede neural</h2>
<p><strong>Parâmetros para rede neural</strong></p>
<h3 id="parâmetros-de-classificação">Parâmetros de Classificação</h3>
<p><strong>Parâmetros de rede neural específicos de classificação</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Componente</td>
<td>choice [&lsquo;class_cnn&rsquo;]</td>
<td>class_cnn</td>
<td>O componente DNN para o modelo de treinamento</td>
</tr>
<tr>
<td>Profundidade da Rede Neural</td>
<td>int [1 - 10]</td>
<td>3</td>
<td>Profundidade (núm camadas) da rede neural</td>
</tr>
<tr>
<td>Largura da rede neural</td>
<td>int [5 - 128]</td>
<td>20</td>
<td>Largura (núm features) da Rede Neural</td>
</tr>
<tr>
<td>Modo de pré-processamento</td>
<td>choice [&lsquo;caffe&rsquo;, &lsquo;tf&rsquo;]</td>
<td>caffe</td>
<td>Função para normalizar imagem</td>
</tr>
<tr>
<td>Função de perda</td>
<td>choice [&lsquo;categorical_crossentropy&rsquo;, &lsquo;binary_crossentropy&rsquo;]</td>
<td>categorical_crossentropy</td>
<td>Função de perda para uso em treinamento</td>
</tr>
<tr>
<td>Funções de métricas</td>
<td>Array of string</td>
<td>[&lsquo;categorical_accuracy&rsquo;]</td>
<td>Funções de métricas para uso em avaliação</td>
</tr>
<tr>
<td>Função Otimizador</td>
<td>choice [&lsquo;adam&rsquo;]</td>
<td>adam</td>
<td>Funções de métricas para uso em avaliação</td>
</tr>
</tbody>
</table>
<h3 id="parâmetros-objectdetection">Parâmetros ObjectDetection</h3>
<p><strong>Parâmetros de rede neural específicos de detecção de objeto</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Componente</td>
<td>choice [&lsquo;objdet&rsquo;]</td>
<td>objdet</td>
<td>O componente DNN para o modelo de treinamento</td>
</tr>
<tr>
<td>Largura da rede neural</td>
<td>int [5 - 128]</td>
<td>20</td>
<td>Largura (núm features) da Rede Neural</td>
</tr>
<tr>
<td>Backbone de rede neural</td>
<td>choice [&lsquo;vgg7&rsquo;, &lsquo;vgg16&rsquo;, &lsquo;vgg19&rsquo;, &lsquo;resnet18&rsquo;, &lsquo;resnet34&rsquo;, &lsquo;resnet50&rsquo;, &lsquo;resnet101&rsquo;, &lsquo;resnet152&rsquo;, &lsquo;mobilenet128&rsquo;, &lsquo;mobilenet160&rsquo;, &lsquo;mobilenet192&rsquo;, &lsquo;mobilenet224&rsquo;, &lsquo;densenet121&rsquo;, &lsquo;densenet169&rsquo;, &lsquo;densenet201&rsquo;]</td>
<td>vgg7</td>
<td>Arquitetura de backbone para rede neural</td>
</tr>
<tr>
<td>Modo de pré-processamento</td>
<td>choice [&lsquo;caffe&rsquo;, &lsquo;tf&rsquo;]</td>
<td>caffe</td>
<td>Função para normalizar imagem</td>
</tr>
<tr>
<td>Sobreposição negativa IoU</td>
<td>number [0.05 - 1.0]</td>
<td>0.3</td>
<td>Valor para sobreposição mínima de caixas negativas</td>
</tr>
<tr>
<td>Sobreposição positiva IoU</td>
<td>number [0.05 - 1.0]</td>
<td>0.45</td>
<td>Valor para sobreposição mínima de caixas positivas</td>
</tr>
</tbody>
</table>
<h4 id="parâmetros-de-âncora">Parâmetros de âncora</h4>
<p><strong>Parâmetros para âncoras de caixas</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tamanhos de caixas</td>
<td>Array of integer</td>
<td>[12, 24, 48, 96, 192]</td>
<td>Tamanho das caixas em cada camada</td>
</tr>
<tr>
<td>Boxes strides</td>
<td>Array of integer</td>
<td>[8, 16, 32, 64, 128]</td>
<td>Strides de caixas em cada camada</td>
</tr>
<tr>
<td>Razões das caixas</td>
<td>Array of number</td>
<td>[0.5, 1, 2]</td>
<td>Proporções (altura / largura) das caixas candidatas</td>
</tr>
<tr>
<td>Escalas de caixas</td>
<td>Array of number</td>
<td>[1, 1.2, 1.5]</td>
<td>Escalas de caixas candidatas</td>
</tr>
</tbody>
</table>
<!-- </parm_table> -->
<h2 id="para-onde-devo-ir-agora">Para onde devo ir agora?</h2>
<ul>
<li><a href="/docs/concepts/training/train_parms">Parâmetros de Treinamento</a>: Parâmetros gerais para treinamento da rede neural</li>
<li><a href="/docs/concepts/training/dnn_parms">Parâmetros de Redes Neurais</a>: Parâmetros específicos da rede neural</li>
<li><a href="/docs/concepts/training/data_augmentation_parms">Parâmetros de Expansão de Dados</a>: Parâmetros para expansão de dados - Data Augmentation</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2869d0ba31e7658c3d9261182907e451">4.5 - Data Augmentation</h1>
    <div class="lead">Expansão de exemplos nos treinamentos</div>
	<p>Para um bom resultado em um treinamento de redes neurais a regra é apresentar um grande número de exemplos diferentes
para que a rede neural possa aprender o padrão nos dados.
Porém, conseguir e anotar milhares de exemplos é uma tarefa árdua e demorada, podendo consumir centenas de horas de um
projeto.</p>
<p>Para facilitar esse processo o Eyeflow.AI oferece uma extensa gama de algorítmos de Expansão de Dados (<em>Data Augmentation</em>).
Trata-se de inserir perturbações nas imagens na hora em que estão sendo apresentadas para a rede neural no treinamento,
forçando assim que a rede neural aprenda a reconhecer os padrões, sem ficar dependendo somente dos exemplos estáticos
que existem no dataset.</p>
<p>Trata-se de alterações diversas como:</p>
<ul>
<li>Alterações na óptica como brilho, contraste, cor, gama</li>
<li>Alterações na forma como rotações, deformações, posições</li>
<li>Alterações na qualidade como desfoque e ruído</li>
</ul>
<blockquote class="note callout">
    <div> <h5 style="color:mediumblue"><i class="fas fa-lightbulb"></i>&nbspImportante!</h5> As alterações nas imagens não podem ser muito altas a ponto de o objeto de interesse não poder ser mais reconhecido.
Para isso o Eyeflow.AI disponibiliza um exemplo dessas alterações para que o operador verifique se as alterações não
estão exageradas. O que não puder ser visto nesses exemplos, não poderá ser aprendido pela rede neural.</div>
</blockquote>
<blockquote class="note callout">
    <div> <h5 style="color:mediumblue"><i class="fas fa-lightbulb"></i>&nbspImportante!</h5> Também não adianta estressar as mudanças e gerar casos que não acontecem na operação real.
De nada adianta apresentar pro aprendizado da rede neural uma imagem invertida, de cabeça para baixo, por exemplo, se
na operação real essa situação nunca irá acontecer.</div>
</blockquote>
<!-- <parm_table> -->
<h2 id="parâmetros-para-expansão-de-dataset">Parâmetros para expansão de dataset</h2>
<p><strong>Parâmetros para expansão de dataset</strong></p>
<h3 id="parâmetros-gerais-de-aumento-de-dados">Parâmetros gerais de aumento de dados</h3>
<p><strong>Parâmetros gerais para todas as transformações</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Interpolação</td>
<td>choice [&lsquo;linear&rsquo;, &lsquo;nearest&rsquo;, &lsquo;cubic&rsquo;, &lsquo;area&rsquo;, &lsquo;lanczos4&rsquo;]</td>
<td>linear</td>
<td>Interpolação para operações de redimensionamento</td>
</tr>
<tr>
<td>Modo de Preenchimento</td>
<td>choice [&lsquo;constant&rsquo;, &lsquo;nearest&rsquo;, &lsquo;reflect&rsquo;, &lsquo;wrap&rsquo;]</td>
<td>constant</td>
<td>Preenchimento de regiões nulas</td>
</tr>
<tr>
<td>Valor da borda</td>
<td>int [0 - 255]</td>
<td>0</td>
<td>A cor para preencher as regiões limítrofes</td>
</tr>
</tbody>
</table>
<h3 id="rotacionar-imagem">Rotacionar imagem</h3>
<p><strong>Rotação aleatória da imagem</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probabilidade</td>
<td>percent 0% - 100%</td>
<td>0.3</td>
<td>Probabilidade de rotação aleatória</td>
</tr>
<tr>
<td>Ângulo mínimo e máximo para rotação</td>
<td>range from -90 to 90</td>
<td>-20 to 20</td>
<td>Rotação aleatória da imagem</td>
</tr>
<tr>
<td>Redimensionar imagem</td>
<td>bool [True - False]</td>
<td>True</td>
<td>Se a imagem deve ser redimensionada em rotação</td>
</tr>
</tbody>
</table>
<h3 id="transladar-imagem">Transladar imagem</h3>
<p><strong>Translação aleatória de imagem</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probabilidade</td>
<td>percent 0% - 100%</td>
<td>0.3</td>
<td>Probabilidade de mudança de translação</td>
</tr>
<tr>
<td>Translação horizontal</td>
<td>range from -0.4 to 0.4</td>
<td>-0.2 to 0.2</td>
<td>mín. e máx. Para translação horizontal</td>
</tr>
<tr>
<td>Tradução vertical mínima e máxima</td>
<td>range from -0.4 to 0.4</td>
<td>-0.2 to 0.2</td>
<td>Porcentagem mínima e máxima para tradução vertical</td>
</tr>
<tr>
<td>Número de Tentativas</td>
<td>int [1 - 6]</td>
<td>3</td>
<td>Número máximo de tentativas sem degenerar caixas</td>
</tr>
</tbody>
</table>
<h3 id="esticar-imagem">Esticar imagem</h3>
<p><strong>Deformação por cisalhamento aleatório da imagem</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probabilidade</td>
<td>percent 0% - 100%</td>
<td>0.3</td>
<td>Probabilidade de mudança de cisalhamento</td>
</tr>
<tr>
<td>Imagem de cisalhamento mínimo e máximo</td>
<td>range from -60 to 60</td>
<td>-10 to 10</td>
<td>Valores mínimo e máximo para deformação por cisalhamento aleatório</td>
</tr>
</tbody>
</table>
<h3 id="imagem-em-escala">Imagem em escala</h3>
<p><strong>Escala aleatória da imagem</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probabilidade</td>
<td>percent 0% - 100%</td>
<td>0.3</td>
<td>Probabilidade de escala aleatória</td>
</tr>
<tr>
<td>Imagem em escala mínima e máxima</td>
<td>range from 0.4 to 1.6</td>
<td>0.8 to 1.2</td>
<td>Valores mínimo e máximo para escala aleatória</td>
</tr>
</tbody>
</table>
<h3 id="virada-aleatória-da-imagem">Virada aleatória da imagem</h3>
<p><strong>Flip Parameters</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probabilidade de inversão horizontal</td>
<td>percent 0% - 100%</td>
<td>0.3</td>
<td>Virada horizontal aleatória da imagem</td>
</tr>
<tr>
<td>Probabilidade de inversão vertical</td>
<td>percent 0% - 100%</td>
<td>0.3</td>
<td>Virada vertical aleatória da imagem</td>
</tr>
</tbody>
</table>
<h3 id="contraste-aleatório">Contraste aleatório</h3>
<p><strong>Mudanças aleatórias no contraste da imagem</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probabilidade</td>
<td>percent 0% - 100%</td>
<td>0.3</td>
<td>Probabilidade de mudança de contraste</td>
</tr>
<tr>
<td>Contraste mínimo e máximo</td>
<td>range from 0.4 to 2</td>
<td>0.8 to 1.2</td>
<td>Valores mínimo e máximo para mudanças no contraste da imagem</td>
</tr>
</tbody>
</table>
<h3 id="brilho-aleatório">Brilho aleatório</h3>
<p><strong>Mudanças aleatórias no brilho da imagem</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probabilidade</td>
<td>percent 0% - 100%</td>
<td>0.3</td>
<td>Probabilidade de mudança de brilho</td>
</tr>
<tr>
<td>Brilho mínimo e máximo</td>
<td>range from -0.6 to 0.6</td>
<td>-0.2 to 0.3</td>
<td>Valores mínimo e máximo para mudanças no brilho da imagem</td>
</tr>
</tbody>
</table>
<h3 id="gama-aleatória">Gama aleatória</h3>
<p><strong>Mudanças aleatórias na gama da imagem</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probabilidade</td>
<td>percent 0% - 100%</td>
<td>0.3</td>
<td>Probabilidade de gama aleatória</td>
</tr>
<tr>
<td>Gama mín. e máx.</td>
<td>range from 0.1 to 12</td>
<td>0.4 to 1.6</td>
<td>Valores mínimo e máximo para mudanças na gama da imagem</td>
</tr>
</tbody>
</table>
<h3 id="saturação-aleatória">Saturação aleatória</h3>
<p><strong>Mudanças aleatórias na saturação da imagem</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probabilidade</td>
<td>percent 0% - 100%</td>
<td>0.3</td>
<td>Probabilidade de mudança de saturação</td>
</tr>
<tr>
<td>Saturação mínima e máxima</td>
<td>range from 0.1 to 2</td>
<td>0.5 to 1.5</td>
<td>Valores mínimo e máximo para mudanças na saturação da imagem</td>
</tr>
</tbody>
</table>
<h3 id="matiz-aleatório">Matiz aleatório</h3>
<p><strong>Mudanças aleatórias no matiz da imagem</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probabilidade</td>
<td>percent 0% - 100%</td>
<td>0.3</td>
<td>Probabilidade de matiz aleatório</td>
</tr>
<tr>
<td>Matiz mín. e máx.</td>
<td>range from -1 to 1</td>
<td>-0.05 to 0.05</td>
<td>Valores mínimo e máximo para mudanças no matiz da imagem</td>
</tr>
</tbody>
</table>
<h3 id="ruído-aleatório">Ruído aleatório</h3>
<p><strong>Inserção aleatória de ruído de imagem</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probabilidade</td>
<td>percent 0% - 100%</td>
<td>0.3</td>
<td>Probabilidade de inserção de ruído</td>
</tr>
<tr>
<td>Método de ruído</td>
<td>choice [&lsquo;gauss&rsquo;, &lsquo;poisson&rsquo;, &lsquo;speckle&rsquo;]</td>
<td>gauss</td>
<td>Método para inserção de ruído</td>
</tr>
</tbody>
</table>
<h3 id="desfoque-aleatório">Desfoque aleatório</h3>
<p><strong>Desfoque aleatório da imagem</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probabilidade</td>
<td>percent 0% - 100%</td>
<td>0.3</td>
<td>Probabilidade de desfoque aleatório</td>
</tr>
<tr>
<td>Tamanho do kernel</td>
<td>choice [3, 5, 7, 9]</td>
<td>5</td>
<td>O tamanho do kernel para desfocar a imagem</td>
</tr>
</tbody>
</table>
<!-- </parm_table> -->
<h2 id="para-onde-devo-ir-agora">Para onde devo ir agora?</h2>
<ul>
<li><a href="/docs/concepts/training/train_parms">Parâmetros de Treinamento</a>: Parâmetros gerais para treinamento da rede neural</li>
<li><a href="/docs/concepts/training/dnn_parms">Parâmetros de Redes Neurais</a>: Parâmetros específicos da rede neural</li>
<li><a href="/docs/concepts/training/data_augmentation_parms">Parâmetros de Expansão de Dados</a>: Parâmetros para expansão de dados - Data Augmentation</li>
</ul>

</div>



    
	
  

    
	
  



          </main>
        </div>
      </div>
      
<footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="User mailing list" aria-label="User mailing list">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://example.org/mail">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://example.org/twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://example.org/stack">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://github.com/siliconlife-ai/eyeflow_sdk">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://example.org/slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Developer mailing list" aria-label="Developer mailing list">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://example.org/mail">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">&copy; 2021 The Docsy Authors Todos os direitos reservados</small>
        <small class="ml-1"><a href="https://policies.google.com/privacy" target="_blank">Política de Privacidade</a></small>
	
		
	
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>











<script src="/js/main.min.5c74b870c6953931a705f390a49c7e4c0a842ec5c83b24354758dd674343ed0d.js" integrity="sha256-XHS4cMaVOTGnBfOQpJx&#43;TAqELsXIOyQ1R1jdZ0ND7Q0=" crossorigin="anonymous"></script>




  </body>
</html>
