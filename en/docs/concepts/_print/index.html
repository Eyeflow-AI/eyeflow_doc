<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.82.0" /><link rel="canonical" type="text/html" href="/en/docs/concepts/">
<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">


<link rel="shortcut icon" href="/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/android-36x36.png" sizes="36x36">
<link rel="icon" type="image/png" href="/favicons/android-48x48.png" sizes="48x48">
<link rel="icon" type="image/png" href="/favicons/android-72x72.png" sizes="72x72">
<link rel="icon" type="image/png" href="/favicons/android-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/favicons/android-144x144.png" sizes="144x144">
<link rel="icon" type="image/png" href="/favicons/android-192x192.png" sizes="192x192">

<title>Concepts | Eyeflow.AI Docs</title><meta property="og:title" content="Concepts" />
<meta property="og:description" content="Main concepts of Eyeflow and Video Analytics
" />
<meta property="og:type" content="website" />
<meta property="og:url" content="/en/docs/concepts/" /><meta property="og:site_name" content="Eyeflow.AI Docs" />

<meta itemprop="name" content="Concepts">
<meta itemprop="description" content="Main concepts of Eyeflow and Video Analytics
"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Concepts"/>
<meta name="twitter:description" content="Main concepts of Eyeflow and Video Analytics
"/>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-00000000-0', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>






<link rel="preload" href="/scss/main.min.ce3463a1ef5c917875e1d7fba7e07a173525e77a633bf30508294531f61e7dd6.css" as="style">
<link href="/scss/main.min.ce3463a1ef5c917875e1d7fba7e07a173525e77a633bf30508294531f61e7dd6.css" rel="stylesheet" integrity="">


<script
  src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
  crossorigin="anonymous"></script>




  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
        <a class="navbar-brand" href="/en/">
		<span class="navbar-logo"><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg></span><span class="text-uppercase font-weight-bold">Eyeflow.AI Docs</span>
	</a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				<a  class="nav-link active" href="/en/docs/" ><span class="active">Documentation</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				<a  class="nav-link" href="/en/blog/" ><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				<a  class="nav-link" href="/en/community/" ><span>Community</span></a>
			</li>
			
			
			
			<li class="nav-item dropdown d-none d-lg-block">
				

<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	English
</a>
<div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/docs/concepts/">Português</a>
	
</div>
			</li>
			
		</ul>
	</div>
	<div class="navbar-nav d-none d-lg-block">
<input type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="off">

</div>
</nav>

    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
          </div>
          <div class="d-none d-xl-block col-xl-2 td-toc d-print-none">
          </div>
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
This the multi-page printable view of this section.
<a href="#" onclick="print();return false;">Click here to print</a>.
</p><p>
<a href="/en/docs/concepts/">Return to the regular view of this page</a>.
</p>
</div>



<h1 class="title">Concepts</h1>
<div class="lead">Main concepts of Eyeflow and Video Analytics</div>




    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-c5d74578d83b9f2882ac6e6033941c0f">Overview</a></li>


    
  
    
    
	
<li>2: <a href="#pg-7d473224ea4bfc39a535883237df5ec8">Flow</a></li>


    
  
    
    
	
<li>3: <a href="#pg-b6a5d2943af2cc53e0d47e249ac80d37">Dataset</a></li>


    
  
    
    
	
<li>4: <a href="#pg-c4d184857c60a3b1e4c1cc383ba30ff0">Training</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>4.1: <a href="#pg-821691bbab1c2ab21c91a76358a6c1c7">Overview</a></li>


    
  
    
    
	
<li>4.2: <a href="#pg-cc7704bf22b8c46dbb695f0b4ca5d35f">Training Dashboard</a></li>


    
  
    
    
	
<li>4.3: <a href="#pg-c31b22b7edbbfe49e4d16d2ccdeaba6b">Training Parameters</a></li>


    
  
    
    
	
<li>4.4: <a href="#pg-12538f171a6b845bb3b653e76783b790">Parameters of Neural Networks</a></li>


    
  
    
    
	
<li>4.5: <a href="#pg-2869d0ba31e7658c3d9261182907e451">Data Augmentation</a></li>


    
  

    </ul>
    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-c5d74578d83b9f2882ac6e6033941c0f">1 - Overview</h1>
    <div class="lead">Main concepts of Eyeflow.AI</div>
	<h2 id="application">Application</h2>
<p>Set of elements that implement a complete Video Analytics process.</p>
<p>When we are faced with a problem that involves vision we start to think about how we can solve it using Eyeflow. At the entrance we have the capture of images by an industrial camera or a cell phone, and at the exit we want to have these images analyzed and interpreted, so that actions can be taken in response. This solution to the problem is what we call Application.</p>
<h2 id="flow">Flow</h2>
<p>Set of chained components that implement an application.</p>
<p>In the process of building an application the goal is to transform a video input (<em>unstructured data</em>) into a programmed action (<em>structured data</em>). In this process we will use neural networks and other chained software components to be able to decompose and interpret the input images and generate the desired outputs.</p>
<h2 id="neural-network">Neural Network</h2>
<p>Advanced mathematical algorithm that allows the computation of unstructured data.</p>
<p>Neural networks are computational elements that have been developed since the 1960s, inspired by scientific research on the functioning of the brain (<em>neurons</em>).</p>
<p>As of 2010, with advances in mathematical processing algorithms and approximation of algebraic functions, together with the increase in the capacity of mathematical processors and lower prices due to their popularization in graphics cards for games (<em>GPU</em>), neural networks have evolved to be used multilayer (<em>Deep Neural Network</em>). In this way, several new applications of this technology began to emerge in the solution of complex problems, mostly related to unstructured data, such as sounds and images. Neural networks are the fundamental components of Flow, as they allow the solution of complex computer vision problems, which previously were not amenable to being solved using traditional algorithms.</p>
<p>Neural networks have the ability to converge mathematically to a computational model that identifies patterns in the data. This ability allowed the emergence of Artificial Intelligence solutions that learn from examples that the user delivers to the network.</p>
<h2 id="dataset">Dataset</h2>
<p>Set of examples that instruct the learning of the neural network.</p>
<p>A neural network is like a black box where we insert examples at the entrance and say what we want to have at the exit. Thus, the user must generate a set of these examples, annotated with the desired output, which will serve as data for training the neural network in the execution of the task.</p>
<h2 id="training">Training</h2>
<p>Computational process to converge a neural network for learning patterns.</p>
<p>After we have an annotated dataset we put the neural network algorithms to process these examples until it learns how to generate the desired output. The training seeks to reduce the error measured between what the neural network presented in the output in relation to the example annotation made by the user. When this error is reduced, the neural network will be ready to process new data.</p>
<h2 id="model">Model</h2>
<p>Neural network trained with a dataset.</p>
<p>After the training process, a model is generated, which can then be used in Flow to process the images and generate the desired outputs.</p>
<h2 id="edge">Edge</h2>
<p>Computational device that performs a flow in production.</p>
<p>After a flow is developed, tested and showing good results, it can be published for execution on a device that will start to work in production, such as the detection of a defect in the manufacturing line.</p>
<h2 id="where-should-i-go-now">Where should I go now?</h2>
<ul>
<li><a href="/docs/Concepts/dataset/">Dataset</a></li>
<li><a href="/docs/Concepts/flow/">Flow</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7d473224ea4bfc39a535883237df5ec8">2 - Flow</h1>
    <div class="lead">Flow diagram for image processing and decision making</div>
	<p>Flow is a program in low-code format, built with code components, oriented to the decomposition of image data (<em>unstructured</em>) into output data (<em>structured</em>).</p>
<p><strong>Flow is structured as an input-output process.</strong></p>
<p>The input is a stream of images, such as:</p>
<ul>
<li>Camera: Industrial, Security, IP</li>
<li>Cell</li>
<li>Video Archive</li>
</ul>
<p>The output can be for any other data system such as:</p>
<ul>
<li>Files: JSON, CSV, TXT</li>
<li>Databases: MongoDB, Postgres</li>
<li>Message queues: RabbitMQ, AMQP</li>
<li>Controllers: PLC, IC, TCP, RS-232</li>
<li>Sending alarms: E-mail, SMS, Notifications</li>
</ul>
<p>As Eyeflow is an extensible platform, there are no limits to component creation. It is possible to quickly create a Python component and use it in Flow to perform the desired function.</p>
<p>Between input and output, image processing is done in Neural Network components. Each of these components will be linked to a dataset, and will use the model trained with the dataset to run in production.</p>
<p>It is also possible to use components that send the images for processing in services of cloud providers. For example, using a component that takes the image of a document and sends it to Google&rsquo;s character recognition API, and then processes the response to scan that document.</p>
<p>Creating a Flow is a very simple process.</p>
<h4 id="click-on-the-flow-tab-in-the-navigation-bar">Click on the Flow tab in the navigation bar</h4>
<p>It will open the screen to load a Flow or Create a new one. Clicking on <strong>New Flow</strong> will open a screen to enter the data. Just fill in the data and that's it, Flow will be created.</p>
<p><img src="/screenshots/pt-br_create_flow.jpg#bordered" alt="Criar Flow" title="Criar Flow"></p>
<p>After creating the Flow we will add some components to create a simple flow that identifies the Pet in the image.</p>
<h3 id="lets-add-3-components-to-this-flow">Let&rsquo;s add 3 components to this Flow:</h3>
<h4 id="heading"></h4>
<p><img src="/screenshots/pt-br_flow_camera_ip.jpg#bordered" alt="Componente Camera IP" title="Componente Camera IP"></p>
<h4 id="heading-1"></h4>
<p><img src="/screenshots/pt-br_flow_roi_cutter.jpg#bordered" alt="Componente ROI Cutter" title="Componente ROI Cutter"></p>
<h4 id="heading-2"></h4>
<p><img src="/screenshots/pt-br_flow_file_save.jpg#bordered" alt="Componente File Save" title="Componente File Save"></p>
<h4 id="connect-the-outputs-of-each-component-to-the-input-of-the-next">Connect the outputs of each component to the input of the next</h4>
<p><img src="/screenshots/pt-br_flow_basic.jpg#bordered" alt="Flow Completo" title="Flow Completo"></p>
<p><strong>Ready! Flow is complete.</strong></p>
<p>One of the great advantages of the EyeFlow.AI platform is that it is a complete integrated environment for the development of Video Analytics applications. The process of neural networks requires a fair amount of annotated examples in order to learn, and the easiest way to get these examples is to use videos.</p>
<p>So, the development process involves a cycle of activities:</p>
<ol>
<li>Test the video in Flow by checking if the application is marking the video correctly</li>
<li>Identify the errors that the application is making and go in the datasets</li>
<li>On the <strong>New Examples</strong> screen, look for frames where the neural network did not correctly identify the object</li>
<li>Add several examples of error (30 to 50 in each cycle is a good number)</li>
<li>Note the new examples by correcting errors</li>
<li>Train the neural network and check the assertiveness indicators to see if the network is learning well</li>
<li>After training, go back to test the video again</li>
</ol>
<p>After having good results in the annotations of the videos, our Flow is ready to publish on the edge.</p>
<h2 id="where-should-i-go-now">Where should I go now?</h2>
<ul>
<li><a href="/docs/concepts/dataset/">Dataset</a></li>
<li><a href="/docs/concepts/training/">Training</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b6a5d2943af2cc53e0d47e249ac80d37">3 - Dataset</h1>
    <div class="lead">Set of annotated examples for training neural networks</div>
	<!-- <blockquote class="note callout">
    <div><strong>Note:</strong> </div>
</blockquote> -->
<p>A dataset is a set of annotated examples that serve to instruct a neural network about what is desired to be recognized in the images. In the dataset set of images, we will have several different examples of the objects we want to identify or recognize. These examples will feed the training of the neural network so that the common pattern between them can be identified.</p>
<h2 id="dataset-types">Dataset types</h2>
<p>Datasets can be of several types. The most used in Eyeflow are:</p>
<ul>
<li><a href="#objectdetection">ObjectDetection</a></li>
<li><a href="#classification">Classification</a></li>
<li><a href="#anomallydetection">AnomallyDetection</a></li>
<li><a href="#instancesegmentation">InstanceSegmentation</a></li>
</ul>
<p>Each type of dataset has a different annotation, and a different type of output.</p>
<h3 id="objectdetection">ObjectDetection</h3>
<p>ObjectDetection datasets are often the most used for their versatility and ease of annotation. The objective is to identify a specific object in a large image, delimiting a rectangular area around the object. Your annotation is usually very simple and quick, as you just click and drag a rectangle (<em>box</em>) and define which class of the object. It is executed by a type of neural network that can be quite optimized for execution at the edge, reaching hundreds of FPS in real time.</p>
<h3 id="classification">Classification</h3>
<p>Classification datasets are the most common, and were the first to appear in the DeepLearning universe. It basically identifies an image as being one of N classes. For example, if you have a list of cat images, you can define the breed of each one, and the neural network will learn to identify the breed in any image.</p>
<h3 id="anomallydetection">AnomallyDetection</h3>
<blockquote class="note callout">
    <div><strong>Note:</strong> This is a dataset still in the experimental phase.</div>
</blockquote>
<p>The AnomallyDetection dataset is designed to identify anomalies in images of the same object. The idea is to feed the dataset with many images of the &quot;correct&quot; object and thus the neural network learn the good pattern. When the neural network is presented to an image of an object that has a difference from the good pattern (anomaly), it will point out the anomaly.</p>
<h3 id="instancesegmentation">InstanceSegmentation</h3>
<blockquote class="note callout">
    <div><strong>Note:</strong> This is a dataset still in the experimental phase.</div>
</blockquote>
<p>InstanceSegmentation is a dataset that has become popular in recent years due to its intense use in autonomous vehicle systems and AI systems for medicine. It is about identifying the object, along with all its outline pixel by pixel. Its output is quite impressive, since it allows you to &quot;cut&quot; the object in detail in the background of the image, however, its annotation is extremely laborious, usually requiring a large team of people to generate an adequate set of annotated images for training.</p>
<h2 id="creating-a-dataset">Creating a dataset</h2>
<h4 id="creating-a-dataset-is-a-very-simple-task-just-click-on-the-side-menu-in-new-dataset">Creating a dataset is a very simple task. Just click on the side menu in <strong>New Dataset</strong>.</h4>
<p><img src="/screenshots/pt-br_create_dataset.jpg#bordered" alt="Criar Dataset" title="Criar Dataset"></p>
<h4 id="a-screen-will-open-to-enter-data-from-the-dataset">A screen will open to enter data from the dataset.</h4>
<p><img src="/screenshots/pt-br_create_dataset_modal.jpg#bordered" alt="Criar Dataset" title="Criar Dataset"></p>
<p>Just fill in the Name and Description, choose the type and define which Application the dataset belongs to. Then you need to add at least one class.</p>
<h2 id="classes">Classes</h2>
<p>The goal of the neural network will be to identify a pattern in the image, and output it in data format. Thus, we created classes in the dataset to be able to use to mark the images, and thus teach the neural network to recognize these patterns and inform us which class was recognized. For example, if we have several images of examples of dogs and cats, and we want to know if it is one or the other in the image, we have created two classes:</p>
<p><img src="/screenshots/pt-br_create_classes_modal.jpg#bordered" alt="Criar Classes" title="Criar Classes"></p>
<p>The colors chosen will help to annotate the dataset, and will appear in the annotation of the videos.</p>
<h2 id="examples">Examples</h2>
<p>Examples are annotated images that will instruct the neural network in learning pattern recognition. We insert several images in the dataset from frames of a video or photos, and then write down in each of these images what is desired to be recognized. The simplest and fastest way to add new examples is to extract sample videos. When playing a video in a Flow, the tool extracts some of the frames from the video and makes them available on the New Examples screen.</p>
<p><img src="/screenshots/pt-br_menu_new_examples.jpg#bordered" alt="Menu Novos Exemplos" title="Menu Novos Exemplos"></p>
<p>On this screen it is also possible to upload new examples from images located on the computer.</p>
<p><img src="/screenshots/pt-br_insert_new_examples.jpg#bordered" alt="Inserir Novos Exemplos" title="Inserir Novos Exemplos"></p>
<blockquote class="note callout">
    <div> <h5 style="color:mediumblue"><i class="fas fa-lightbulb"></i>&nbspImportant!</h5> For a neural network to learn well it is important that there is a good diversity of examples. Inserting several very similar images, or inserting many images of a class and few of the others will cause the dataset to become unbalanced and the network will not learn well</div>
</blockquote>
<h2 id="annotation">Annotation</h2>
<p>Annotation is the process where the user marks the object in the image that he wants to be identified, thus generating an example that will be used in the training of the neural network.</p>
<p>The annotations are different for each type of dataset. In a dataset <a href="#objectdetection">ObjectDetection</a></p>
<p><img src="/screenshots/pt-br_annotate_example.jpg#bordered" alt="Anotar Exemplo" title="Anotar Exemplo"></p>
<h2 id="where-should-i-go-now">Where should I go now?</h2>
<ul>
<li><a href="/docs/concepts/training/">Training</a></li>
<li><a href="/docs/concepts/flow/">Flow</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c4d184857c60a3b1e4c1cc383ba30ff0">4 - Training</h1>
    <div class="lead">Learning process of neural networks</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-821691bbab1c2ab21c91a76358a6c1c7">4.1 - Overview</h1>
    <div class="lead">Training overview</div>
	<p>Training is the process where the examples from the dataset are fed into the neural network to learn the patterns and generate the model. It is a complex mathematical / algorithmic process, which requires a large computational capacity and needs to run on servers with GPU or TPU graphics cards in the case of the Google cloud.</p>
<p>This process is performed in an iterative way, seeking to minimize the error measured between what the neural network presented in the output and the annotation made in the example by the user. Throughout the error minimization process, the neural network's assertiveness is measured, so that the user can verify if the neural network has really learned to identify the patterns.</p>
<p>The process of training neural networks is quite complex, and mastering this process well takes a lot of time, study and experimentation.</p>
<p>Fortunately, Eyeflow.AI offers a complete environment to accelerate and automate all tasks for the development of neural networks, and has suggestions for construction and parameterization, the result of long years of experience using neural networks to solve real problems.</p>
<h2 id="important-concepts">Important concepts</h2>
<h3 id="number-of-examples">Number of examples</h3>
<p>The number of examples of a dataset is a very important item for the effectiveness of the detections. To perform a training of a neural network it is necessary at least 10 examples, and a good dataset for production will require about 1000 examples. However, there is no point in simply filling up a dataset of examples without testing its effectiveness with real cases, as an excess of similar examples can unbalance learning and cause your model to be biased and not perform well in production.</p>
<p>Another important point is that a similar number of examples from each class must be maintained, also to avoid imbalance, and consequently bias, of the model. If the dataset has many examples of a single class and few of the others, it will tend to detect everything as the dominant class.</p>
<p>The best strategy for adding new examples to the dataset is to test it after training and check for errors, to add only the new examples that showed errors, and then correct them and do a new training. At each of these iterations, the ideal is to add 30 to 50 new examples, and from there the training will quickly increase the assertiveness of detection.</p>
<p>After testing with videos and publishing in production, Eyeflow offers a mechanism for collecting new examples from the edge, which will serve to check if the model is having a good assertiveness in real cases, and then errors can be added to continue the process of improving the dataset.</p>
<h3 id="quality-of-the-examples">Quality of the examples</h3>
<p>If quantity is important, quality is critical to ensuring that the neural network will learn to detect patterns.</p>
<p>To guarantee the quality of the examples, some points must be observed:</p>
<ul>
<li>The object to be detected must be clearly visible in the image. If there are doubts to note the example then the neural network itself will have difficulties in learning from that example. - For an ObjectDetection dataset, the boxes cannot be too small or too large. The augmentation tests will indicate whether the boxes are being well detected. - For a Classification dataset, the examples need to be quite different between classes. It will be difficult to detect a difference between classes if that difference is just a small detail in the total image. - There should be a good variety in the examples. The addition of many very similar examples will cause the model to be skewed to detect only that type of image or class.</li>
</ul>
<h3 id="validation-and-testing">Validation and Testing</h3>
<p>In the training process, the dataset is divided into 3 groups:</p>
<ul>
<li>Training</li>
<li>Validation</li>
<li>Test</li>
</ul>
<p>The separation of the examples for the groups is done at random, based on the parameters defined for the training.</p>
<p>The <strong>Training</strong> group is what will be effectively used to train the neural network. It should have the largest number of examples, with at least 80% of the examples recommended.</p>
<p>The <strong>Validation</strong> group is used to test the model at each end of the season, and is used to check if the training is evolving, or has become stagnant or addictive (<em>overfitting</em>). It is recommended that 10% of the examples be up to a maximum of 100 examples.</p>
<p>The <strong>Test</strong> group is used to test the model at the end of training. As these examples were never presented to the dataset, they serve as a final assessment of learning, defining the assertiveness that the model presents. It is an indicator of whether the neural network has effectively learned to recognize patterns. It is recommended that 10% of the examples be up to a maximum of 100 examples.</p>
<blockquote class="note callout">
    <div><strong>Note:</strong> The main objective of a neural network training is to ensure that the final model will <strong>generalize</strong>, it means that you will learn from the pattern the examples so that you can extrapolate to new examples in real situations. Often the result is good in training, but poor in production, indicating that the model did not generalize well. In these situations, you should collect more examples of errors in production to add to the dataset.</div>
</blockquote>
<h3 id="loss-and-val-loss">Loss and Val Loss</h3>
<p>Loss is the measure of error observed in the training process between what the neural network detected (<em>predict</em>) and what should be the correct output according to the annotation (<em>ground thruth</em>). The mathematical / algorithmic training process of neural networks seeks to minimize this error.</p>
<p>Val Loss is the same measure, only done in the validation group at the end of a season. It serves to monitor the evolution of training at each training cycle.</p>
<h3 id="map-and-accuracy">mAP and Accuracy</h3>
<p>The training quality indicator is a measure of correctness that takes into account all the elements of output. Each type of dataset has its standard measure of assertiveness.</p>
<p><em>Datasets</em> ObjectDetection <em>usually use<strong>mAP</strong>(</em> Mean Average Precision*) as an assertiveness indicator, as it measures how much the output box encompasses the object well, indicating the correct class, by the average of all classes. *Datasets* Classification *usually use**Accuracy**, which is the percentage of examples in which the neural network correctly predicted which class the object belongs to.</p>
<h3 id="overfitting">Overfitting</h3>
<p>It is a phenomenon that occurs with an increase in training. It occurs in training when we see <em>Loss</em>continue to fall, but* Val Loss * goes up instead of going down. This indicates that the neural network has become &quot;addicted&quot; to the training examples and has stopped generalizing to new examples, that is, it has learned a lot about the training examples, but is unable to get new examples right.</p>
<h3 id="expanding-examples---data-augmentation">Expanding examples - Data Augmentation</h3>
<p>It is a long and repetitive job to write down the examples of a dataset, but it is the most important task to be able to use a neural network in a real application. Many AI projects stagnate in the part where they need to gather many thousands of annotated examples to start having good results with neural networks, but the experience with Eyeflow.AI indicates that it is possible to have good results with just 300 examples in a dataset, and great results with just 1000 examples. One of the key aspects of this performance is in Data Augmentation.</p>
<p>Data Augmentation is the process of inserting disturbances in the images of the examples, at the time of training, to force the neural network to learn the patterns of the objects, and not to be &quot;addicted&quot; to the few examples that were presented.</p>
<p>Eyeflow.AI offers a wide range of algorithms for Data Augmentation, ranging from geometric and photometric distortions in images to the insertion of noise or strange elements.</p>
<p>The important point to note here is that Data Augumentation should not be used to modify the examples, leaving them very different from the situations that will be encountered in the real environment, as this will unnecessarily hamper the learning of the network. For example, it would not make sense in a car dataset to insert the <em>Flip Vertical</em>, as we will not find a situation with the car upside down.</p>
<h3 id="synthetic-datasets">Synthetic Datasets</h3>
<p>In the same way that Data Augumentation helps to improve the learning of the network with few examples, a Synthetic Dataset helps to generate examples automatically for the training of the network.</p>
<p>There are few situations in which you can use synthetic datasets, one of them is for character recognition (<em>OCR</em>) for example, but when it is possible the gains are enormous, as you can generate an extremely robust and effective model.</p>
<p>Eyeflow.AI is an expandable platform, which allows the user to create their own neural network components and generate synthetic datasets.</p>
<h2 id="where-should-i-go-now">Where should I go now?</h2>
<ul>
<li><a href="/docs/concepts/training/train_parms">Training Parameters</a></li>
<li><a href="/docs/concepts/training/dnn_parms">Parameters of Neural Networks</a></li>
<li><a href="/docs/concepts/training/data_augmentation_parms">Data Expansion Parameters</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-cc7704bf22b8c46dbb695f0b4ca5d35f">4.2 - Training Dashboard</h1>
    <div class="lead">Training overview</div>
	<p>The Training Dashboard gives an overview of the entire training process. On this screen we can monitor the training in real time, as well as evaluate the history of previous training and edit various parameters.</p>
<p><img src="/screenshots/pt-br_dashboard_overview.jpg#bordered" alt="Overview" title="Visão Geral"></p>
<h2 id="indicators">Indicators</h2>
<p>In these tables are the indicators of the selected training. The date that was performed, duration, and final results of the training. We also have a button bar with actions such as <em>Download</em>and* Publication * of the model, visualization of parameters and visualization of test images.</p>
<p><img src="/screenshots/pt-br_dashboard_kpis.jpg#bordered" alt="Indicators" title="Indicadores"></p>
<h3 id="final-training-results">Final training results</h3>
<p><img src="/screenshots/pt-br_dashboard_final_results.jpg#bordered" alt="Resultados Finais" title="Resultados Finais"></p>
<h3 id="sample-expansion-test-image---data-augmentation">Sample expansion test image - Data Augmentation</h3>
<p><img src="/screenshots/pt-br_dashboard_test_augmentation.jpg#bordered" alt="Teste de Expansão" title="Teste de Expansão"></p>
<h3 id="end-of-training-test-image">End of training test image</h3>
<p><img src="/screenshots/pt-br_dashboard_test_final.jpg#bordered" alt="Teste Final" title="Teste Final"></p>
<h2 id="graphics">Graphics</h2>
<p>The graphs give an idea of the training progression, either in real time or in history.</p>
<p><strong>Loss</strong>and<strong>Val Loss</strong> indicate the error measured in the training, and are common indicators for all training. They must be descending curves that approach 0. The lower the Loss, the less the neural network is missing.</p>
<p><img src="/screenshots/pt-br_dashboard_graphs.jpg#bordered" alt="Graphics" title="Gráficos"></p>
<p>There are also graphs of neural network efficacy, and for these, the bigger the better.</p>
<ul>
<li><strong>mAP</strong>for<em>ObjectDetection</em> datasets. It is generally considered good above 0.6.</li>
<li><strong>Accuracy</strong>for<em>Classification</em> datasets. It is considered good above 90%.</li>
</ul>
<h2 id="training-history">Training History</h2>
<p>It is also possible to check the previous trainings and thus have insights on whether the actions that were taken in the construction of the dataset and parameterization of the training had positive or negative effects.</p>
<p><img src="/screenshots/pt-br_dashboard_history.jpg#bordered" alt="Histórico" title="Histórico"></p>
<h2 id="where-should-i-go-now">Where should I go now?</h2>
<ul>
<li><a href="/docs/concepts/dataset/">Dataset</a></li>
<li><a href="/docs/concepts/training/">Training</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c31b22b7edbbfe49e4d16d2ccdeaba6b">4.3 - Training Parameters</h1>
    <div class="lead">Training control</div>
	<h2 id="iterations">Iterations</h2>
<p>The training process is iterative, that is, carried out in cycles. Each cycle is called <strong>Season</strong>. In each epoch all examples are presented for the neural network, so that the patterns are learned. The training algorithm will then measure the error (<em>Loss</em>) and adjust the neural network to minimize it.</p>
<p>It is common to think that with more times the network will learn more, but in practice it is not exactly like that. Depending on the quantity / quality of the examples, a point is reached where the error no longer decreases and learning stagnates. Another common occurrence is that the error (<em>Loss</em>) continues to decrease, but the <em>Val Loss</em>starts to increase. This phenomenon is known as* Overfitting * and it means that the neural network has become addicted to the training examples and is no longer able to generalize to new examples.</p>
<p>The recommendation is to set 5 training periods in the beginning while the dataset has less than 100 examples, and then go up.</p>
<p>In the sequence, we have several other parameters that govern the training process and all can influence positively or negatively on the final result of the network learning. Do not be frightened by the quantity, nor by the complexity of them, it is natural to take a long time to acquire mastery over the whole process.</p>
<p>Rest assured, Eyeflow.AI has a great set of defaults for the parameters that solve the needs of most of the needs. In addition, our team is available to answer questions and give tips in our Forum.</p>
<!-- <parm_table> -->
<h2 id="train-parameters">Train Parameters</h2>
<p><strong>Parameters for Neural Network Training</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Epochs</td>
<td>int [1 - ]</td>
<td>5</td>
<td>Number of epochs for training</td>
</tr>
<tr>
<td>Steps per Epoch</td>
<td>int [50 - ]</td>
<td>100</td>
<td>Number of Steps for training in each Epoch</td>
</tr>
<tr>
<td>Batch Size</td>
<td>int [1 - 64]</td>
<td>10</td>
<td>Number of examples in each step</td>
</tr>
<tr>
<td>Val Size</td>
<td>number [0.01 - 0.9]</td>
<td>0.1</td>
<td>Percent of examples selected for Validation</td>
</tr>
<tr>
<td>Test Size</td>
<td>number [0.01 - 0.9]</td>
<td>0.1</td>
<td>Percent of examples selected for Final Test</td>
</tr>
<tr>
<td>Confidence Threshold</td>
<td>number [0.05 - 1.0]</td>
<td>0.6</td>
<td>Minimum confidence threshold for valid detection</td>
</tr>
<tr>
<td>IoU Detection Threshold</td>
<td>number [0.05 - 1.0]</td>
<td>0.45</td>
<td>Minimum threshold for IoU detection</td>
</tr>
<tr>
<td>Maximum Boxes</td>
<td>int [0 - 300]</td>
<td>30</td>
<td>Maximum number of boxes in detection</td>
</tr>
<tr>
<td>Expand Boxes</td>
<td>number [0 - 2]</td>
<td>0</td>
<td>Percent to expand size of boxes in detection</td>
</tr>
</tbody>
</table>
<h3 id="input-resolution">Input Resolution</h3>
<p><strong>Input image dimensions</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Minimum Side</td>
<td>int [20 - ]</td>
<td>50</td>
<td>Size of the smaller side</td>
</tr>
<tr>
<td>Maximum Side</td>
<td>int [20 - ]</td>
<td>80</td>
<td>Size of the bigger side</td>
</tr>
<tr>
<td>Channels</td>
<td>choice [1, 3]</td>
<td>1</td>
<td>Color channels</td>
</tr>
</tbody>
</table>
<h3 id="optimizer-parameters">Optimizer Parameters</h3>
<p><strong>Parameters for Train Optimizer</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>beta_2</td>
<td>number [0.1 - 1.0]</td>
<td>0.999</td>
<td>Beta 2</td>
</tr>
<tr>
<td>beta_1</td>
<td>number [0.1 - 1.0]</td>
<td>0.9</td>
<td>Beta 1</td>
</tr>
<tr>
<td>Learning Rate</td>
<td>number [1e-06 - 0.1]</td>
<td>0.001</td>
<td>Optimizer Learning Rate</td>
</tr>
<tr>
<td>amsgrad</td>
<td>bool [True - False]</td>
<td>False</td>
<td>AMSGrad</td>
</tr>
</tbody>
</table>
<h3 id="early-stopping">Early Stopping</h3>
<p><strong>Early stopping for training</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Patience</td>
<td>int [1 - ]</td>
<td>5</td>
<td>Num of epochs to wait for progress</td>
</tr>
<tr>
<td>Monitor variable</td>
<td>choice [&lsquo;val_loss&rsquo;, &lsquo;loss&rsquo;, &lsquo;categorical_accuracy&rsquo;, &lsquo;val_categorical_accuracy&rsquo;]</td>
<td>val_loss</td>
<td>Variable to monitor progress</td>
</tr>
<tr>
<td>Minimum Delta</td>
<td>number [0 - ]</td>
<td>0.01</td>
<td>The minimum variantion in variable</td>
</tr>
<tr>
<td>Evaluation mode</td>
<td>choice [&lsquo;min&rsquo;, &lsquo;max&rsquo;, &lsquo;auto&rsquo;]</td>
<td>max</td>
<td>Monitor decrement or increment of variable value</td>
</tr>
</tbody>
</table>
<h3 id="reduce-lr-on-plateau">Reduce LR on plateau</h3>
<p><strong>Reduce Learning Rate on plateau</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Patience</td>
<td>int [1 - ]</td>
<td>4</td>
<td>Num of epochs to wait for progress</td>
</tr>
<tr>
<td>Monitor variable</td>
<td>choice [&lsquo;val_loss&rsquo;, &lsquo;loss&rsquo;, &lsquo;categorical_accuracy&rsquo;, &lsquo;val_categorical_accuracy&rsquo;]</td>
<td>val_loss</td>
<td>Variable to monitor for progress</td>
</tr>
<tr>
<td>Minimum Delta</td>
<td>number [0 - ]</td>
<td>0.01</td>
<td>The minimum variantion in variable</td>
</tr>
<tr>
<td>Reducing factor</td>
<td>number [0.1 - 0.9]</td>
<td>0.5</td>
<td>Ammount to reduce</td>
</tr>
<tr>
<td>Cooldown</td>
<td>number [0 - ]</td>
<td>0</td>
<td>Cool Down</td>
</tr>
</tbody>
</table>
<h3 id="save-checkpoint">Save Checkpoint</h3>
<p><strong>Trigger to save model training progress</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Monitor variable</td>
<td>choice [&lsquo;val_loss&rsquo;, &lsquo;loss&rsquo;, &lsquo;categorical_accuracy&rsquo;, &lsquo;val_categorical_accuracy&rsquo;]</td>
<td>val_loss</td>
<td>Variable to monitor for saving</td>
</tr>
<tr>
<td>Evaluation mode</td>
<td>choice [&lsquo;min&rsquo;, &lsquo;max&rsquo;, &lsquo;auto&rsquo;]</td>
<td>min</td>
<td>Save on decrement or increment of variable value</td>
</tr>
</tbody>
</table>
<!-- </parm_table> -->
<h2 id="where-should-i-go-now">Where should I go now?</h2>
<ul>
<li><a href="/docs/concepts/training/train_parms">Training Parameters</a></li>
<li><a href="/docs/concepts/training/dnn_parms">Parameters of Neural Networks</a></li>
<li><a href="/docs/concepts/training/data_augmentation_parms">Data Expansion Parameters</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-12538f171a6b845bb3b653e76783b790">4.4 - Parameters of Neural Networks</h1>
    <div class="lead">Control of neural networks</div>
	<p>In this set of parameters, aspects related to the architecture of neural networks that will learn from the dataset examples in training are controlled.</p>
<p>Each type of neural network has its own architecture, weight and performance parameters. Eyeflow.AI is an extensible platform that allows you to work with the most diverse architectures. However, our experience of several projects has already taught us about various architectures that work well in production, and it is this experience that we seek to bring to the platform, and thus simplify the lives of users.</p>
<p>In this Beta phase we have 2 main components that we use to solve all the problems that we have encountered.</p>
<!-- <parm_table> -->
<h2 id="neural-network-parameters">Neural Network Parameters</h2>
<p><strong>Parameters for Neural Network</strong></p>
<h3 id="classification-parameters">Classification Parameters</h3>
<p><strong>Classification Specific Neural Network Parameters</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Component</td>
<td>choice [&lsquo;class_cnn&rsquo;]</td>
<td>class_cnn</td>
<td>The DNN component for train model</td>
</tr>
<tr>
<td>Neural Network Depth</td>
<td>int [1 - ]</td>
<td>3</td>
<td>Depth (num layers) of Neural Network</td>
</tr>
<tr>
<td>Neural Network Width</td>
<td>int [5 - ]</td>
<td>20</td>
<td>Wide (num features) of Neural Network</td>
</tr>
<tr>
<td>Preprocess Mode</td>
<td>choice [&lsquo;caffe&rsquo;, &lsquo;tf&rsquo;]</td>
<td>caffe</td>
<td>Function for image normalize</td>
</tr>
<tr>
<td>Loss Function</td>
<td>choice [&lsquo;categorical_crossentropy&rsquo;, &lsquo;binary_crossentropy&rsquo;]</td>
<td>categorical_crossentropy</td>
<td>Loss function for use in training</td>
</tr>
<tr>
<td>Metrics Functions</td>
<td>Array of string</td>
<td>[&lsquo;categorical_accuracy&rsquo;]</td>
<td>Metrics functions for use in evaluation</td>
</tr>
<tr>
<td>Optimizer Function</td>
<td>choice [&lsquo;adam&rsquo;]</td>
<td>adam</td>
<td>Optimizer function for use in training</td>
</tr>
</tbody>
</table>
<h3 id="objectdetection-parameters">ObjectDetection Parameters</h3>
<p><strong>ObjectDetection Specific Neural Network Parameters</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Component</td>
<td>choice [&lsquo;objdet&rsquo;, &lsquo;retinanet&rsquo;]</td>
<td>objdet</td>
<td>The DNN component for train model</td>
</tr>
<tr>
<td>Neural Network Width</td>
<td>int [5 - ]</td>
<td>20</td>
<td>Wide (num features) of Neural Network</td>
</tr>
<tr>
<td>Neural Network backbone</td>
<td>choice [&lsquo;vgg7&rsquo;, &lsquo;vgg16&rsquo;, &lsquo;vgg19&rsquo;, &lsquo;resnet18&rsquo;, &lsquo;resnet34&rsquo;, &lsquo;resnet50&rsquo;, &lsquo;resnet101&rsquo;, &lsquo;resnet152&rsquo;, &lsquo;mobilenet128&rsquo;, &lsquo;mobilenet160&rsquo;, &lsquo;mobilenet192&rsquo;, &lsquo;mobilenet224&rsquo;, &lsquo;densenet121&rsquo;, &lsquo;densenet169&rsquo;, &lsquo;densenet201&rsquo;]</td>
<td>vgg7</td>
<td>Backbone architecture to Neural Network</td>
</tr>
<tr>
<td>Preprocess Mode</td>
<td>choice [&lsquo;caffe&rsquo;, &lsquo;tf&rsquo;]</td>
<td>caffe</td>
<td>Function for image normalize</td>
</tr>
<tr>
<td>IoU negative overlap</td>
<td>number [0.05 - 1.0]</td>
<td>0.3</td>
<td>Value for minimum overlap of negative boxes</td>
</tr>
<tr>
<td>IoU positive overlap</td>
<td>number [0.05 - 1.0]</td>
<td>0.45</td>
<td>Value for minimum overlap of positive boxes</td>
</tr>
</tbody>
</table>
<h4 id="anchor-parmeters">Anchor parmeters</h4>
<p><strong>Parameters for boxes anchors</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Boxes sizes</td>
<td>Array of integer</td>
<td>[12, 24, 48, 96, 192]</td>
<td>Size of boxes in each layer</td>
</tr>
<tr>
<td>Boxes strides</td>
<td>Array of integer</td>
<td>[8, 16, 32, 64, 128]</td>
<td>Strides of boxes in each layer</td>
</tr>
<tr>
<td>Boxes ratios</td>
<td>Array of number</td>
<td>[0.5, 1, 2]</td>
<td>Ratios (height / width) of candidate boxes</td>
</tr>
<tr>
<td>Boxes scales</td>
<td>Array of number</td>
<td>[1, 1.2, 1.5]</td>
<td>Scales of candidate boxes</td>
</tr>
</tbody>
</table>
<!-- </parm_table> -->
<h2 id="where-should-i-go-now">Where should I go now?</h2>
<ul>
<li><a href="/docs/concepts/training/train_parms">Training Parameters</a></li>
<li><a href="/docs/concepts/training/dnn_parms">Parameters of Neural Networks</a></li>
<li><a href="/docs/concepts/training/data_augmentation_parms">Data Expansion Parameters</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2869d0ba31e7658c3d9261182907e451">4.5 - Data Augmentation</h1>
    <div class="lead">Expanding training examples</div>
	<p>For a good result in neural network training, the rule is to present a large number of different examples so that the neural network can learn the pattern in the data. However, getting and writing down thousands of examples is an arduous and time-consuming task, and can consume hundreds of hours of a project.</p>
<p>To facilitate this process, Eyeflow.AI offers an extensive range of Data Expansion algorithms (<em>Data Augmentation</em>). It is about inserting disturbances in the images when they are being presented to the neural network in training, thus forcing the neural network to learn to recognize the patterns, without depending only on the static examples that exist in the dataset.</p>
<p>These are several changes such as:</p>
<ul>
<li>Changes in optics such as brightness, contrast, color, gamma</li>
<li>Changes in the way rotations, deformations, positions</li>
<li>Changes in quality such as blur and noise</li>
</ul>
<blockquote class="note callout">
    <div> <h5 style="color:mediumblue"><i class="fas fa-lightbulb"></i>&nbspImportant!</h5> The changes in the images cannot be so high that the object of interest can no longer be recognized. For this, Eyeflow.AI provides an example of these changes for the operator to verify that the changes are not exaggerated. What cannot be seen in these examples, cannot be learned by the neural network.</div>
</blockquote>
<blockquote class="note callout">
    <div> <h5 style="color:mediumblue"><i class="fas fa-lightbulb"></i>&nbspImportant!</h5> It is also useless to stress the changes and generate cases that do not happen in the real operation. It is useless to present an inverted image for the learning of the neural network, upside down, for example, if in real operation this situation will never happen.</div>
</blockquote>
<!-- <parm_table> -->
<h2 id="data-augmentation-parameters">Data augmentation parameters</h2>
<p><strong>Parameters for Images Data Augmentation</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Interpolation</td>
<td>choice [&lsquo;linear&rsquo;, &lsquo;nearest&rsquo;, &lsquo;cubic&rsquo;, &lsquo;area&rsquo;, &lsquo;lanczos4&rsquo;]</td>
<td>linear</td>
<td>Interpolation for resize operations</td>
</tr>
<tr>
<td>Fill Mode</td>
<td>choice [&lsquo;constant&rsquo;, &lsquo;nearest&rsquo;, &lsquo;reflect&rsquo;, &lsquo;wrap&rsquo;]</td>
<td>constant</td>
<td>Fill of null regions</td>
</tr>
<tr>
<td>Border Value</td>
<td>int [0 - 255]</td>
<td>0</td>
<td>The color to fill border regions</td>
</tr>
<tr>
<td>Horizontal flip of image</td>
<td>number [0 - 1.0]</td>
<td>0.3</td>
<td>Random horizontal flip of image</td>
</tr>
<tr>
<td>Vertical flip of image</td>
<td>number [0 - 1.0]</td>
<td>0.3</td>
<td>Random vertical flip of image</td>
</tr>
</tbody>
</table>
<h3 id="rotate-image">Rotate image</h3>
<p><strong>Random rotation of image</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability</td>
<td>number [0 - 1.0]</td>
<td>0.3</td>
<td>Probability of random rotation</td>
</tr>
<tr>
<td>Maximum angle</td>
<td>number [0 - 180]</td>
<td>60</td>
<td>Maximum angle for rotation clockwise</td>
</tr>
<tr>
<td>Minimum angle</td>
<td>number [-180 - 0]</td>
<td>-60</td>
<td>Minimum angle for rotation counter-clockwise</td>
</tr>
<tr>
<td>Rescale image</td>
<td>bool [True - False]</td>
<td>True</td>
<td>If image must be reescaled in rotation</td>
</tr>
</tbody>
</table>
<h3 id="translate-image">Translate image</h3>
<p><strong>Random translation of image</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability</td>
<td>number [0 - 1.0]</td>
<td>0.3</td>
<td>Probability of random translation</td>
</tr>
<tr>
<td>Horizontal translation</td>
<td>Array of number</td>
<td>[0.03, 0.2]</td>
<td>Minimum &amp; Maximum for horizontal translation</td>
</tr>
<tr>
<td>Vertical translation</td>
<td>Array of number</td>
<td>[0.03, 0.2]</td>
<td>Minimum &amp; Maximum for vertical translation</td>
</tr>
<tr>
<td>Number of trials</td>
<td>int [1 - ]</td>
<td>3</td>
<td>Maximum number of trials without degeneration boxes</td>
</tr>
</tbody>
</table>
<h3 id="shear-image">Shear image</h3>
<p><strong>Random shear deformation of image</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability</td>
<td>number [0 - 1.0]</td>
<td>0.3</td>
<td>Probability of random shear</td>
</tr>
<tr>
<td>Shear Minimum</td>
<td>number [-180 - 0]</td>
<td>-10</td>
<td>Minimum value for shear angle in degrees</td>
</tr>
<tr>
<td>Shear Maximum</td>
<td>number [0 - 180]</td>
<td>10</td>
<td>Maximum value for shear angle in degrees</td>
</tr>
</tbody>
</table>
<h3 id="scale-image">Scale image</h3>
<p><strong>Random scale of image</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability</td>
<td>number [0 - 1.0]</td>
<td>0.3</td>
<td>Probability of random scale</td>
</tr>
<tr>
<td>Minimum scale</td>
<td>number [0.1 - ]</td>
<td>0.8</td>
<td>Minimum proportion value for scale</td>
</tr>
<tr>
<td>Maximum scale</td>
<td>number [1.0 - ]</td>
<td>1.2</td>
<td>Maximum proportion value for scale</td>
</tr>
</tbody>
</table>
<h3 id="random-contrast">Random contrast</h3>
<p><strong>Random changes in contrast of image</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability</td>
<td>number [0 - 1.0]</td>
<td>0.3</td>
<td>Probability of random contrast</td>
</tr>
<tr>
<td>Minimum contrast</td>
<td>number [0.1 - 1.0]</td>
<td>0.8</td>
<td>Minimum contrast change</td>
</tr>
<tr>
<td>Maximum contrast</td>
<td>number [1.0 - 2.0]</td>
<td>1.2</td>
<td>Maximum contrast change</td>
</tr>
</tbody>
</table>
<h3 id="random-brightness">Random brightness</h3>
<p><strong>Random changes in brightness of image</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability</td>
<td>number [0 - 1.0]</td>
<td>0.3</td>
<td>Probability of random brightness</td>
</tr>
<tr>
<td>Minimum brightness</td>
<td>number [-1.0 - 0]</td>
<td>-0.2</td>
<td>Minimum brightness change</td>
</tr>
<tr>
<td>Maximum brightness</td>
<td>number [0 - 1.0]</td>
<td>0.3</td>
<td>Maximum brightness change</td>
</tr>
</tbody>
</table>
<h3 id="random-gamma">Random gamma</h3>
<p><strong>Random changes in gamma of image</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability</td>
<td>number [0 - 1.0]</td>
<td>0.3</td>
<td>Probability of random gamma</td>
</tr>
<tr>
<td>Minimum gamma</td>
<td>number [0.1 - 1.0]</td>
<td>0.4</td>
<td>Minimum gamma change</td>
</tr>
<tr>
<td>Maximum gamma</td>
<td>number [0 - 12.0]</td>
<td>1.6</td>
<td>Maximum gamma change</td>
</tr>
</tbody>
</table>
<h3 id="random-saturation">Random saturation</h3>
<p><strong>Random changes in saturation of image</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability</td>
<td>number [0 - 1.0]</td>
<td>0.3</td>
<td>Probability of random saturation</td>
</tr>
<tr>
<td>Minimum saturation</td>
<td>number [0.1 - 1.0]</td>
<td>0.95</td>
<td>Minimum saturation change</td>
</tr>
<tr>
<td>Maximum saturation</td>
<td>number [1.0 - 2.0]</td>
<td>1.05</td>
<td>Maximum saturation change</td>
</tr>
</tbody>
</table>
<h3 id="random-hue">Random hue</h3>
<p><strong>Random changes in hue of image</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability</td>
<td>number [0 - 1.0]</td>
<td>0.3</td>
<td>Probability of random hue</td>
</tr>
<tr>
<td>Minimum hue</td>
<td>number [-1.0 - 0]</td>
<td>-0.05</td>
<td>Minimum hue change</td>
</tr>
<tr>
<td>Maximum hue</td>
<td>number [0 - 1.0]</td>
<td>0.05</td>
<td>Maximum hue change</td>
</tr>
</tbody>
</table>
<h3 id="random-noise">Random noise</h3>
<p><strong>Random insert of noise of image</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability</td>
<td>number [0 - 1.0]</td>
<td>0.3</td>
<td>Probability of random noise</td>
</tr>
<tr>
<td>Noise method</td>
<td>choice [&lsquo;gauss&rsquo;, &lsquo;poisson&rsquo;, &lsquo;speckle&rsquo;]</td>
<td>gauss</td>
<td>Method for noise insertion</td>
</tr>
</tbody>
</table>
<h3 id="random-blur">Random blur</h3>
<p><strong>Random blur of image</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability</td>
<td>number [0 - 1.0]</td>
<td>0.3</td>
<td>Probability of random blur</td>
</tr>
<tr>
<td>Kernel size</td>
<td>choice [3, 5, 7, 9]</td>
<td>5</td>
<td>The size of kernel to blur image</td>
</tr>
</tbody>
</table>
<!-- </parm_table> -->
<h2 id="where-should-i-go-now">Where should I go now?</h2>
<ul>
<li><a href="/docs/concepts/training/train_parms">Training Parameters</a></li>
<li><a href="/docs/concepts/training/dnn_parms">Parameters of Neural Networks</a></li>
<li><a href="/docs/concepts/training/data_augmentation_parms">Data Expansion Parameters</a></li>
</ul>

</div>



    
	
  

    
	
  



          </main>
        </div>
      </div>
      
<footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="User mailing list" aria-label="User mailing list">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://example.org/mail">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://example.org/twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://example.org/stack">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://github.com/siliconlife-ai/eyeflow_sdk">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://example.org/slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Developer mailing list" aria-label="Developer mailing list">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://example.org/mail">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">&copy; 2021 The Docsy Authors All Rights Reserved</small>
        <small class="ml-1"><a href="https://policies.google.com/privacy" target="_blank">Privacy Policy</a></small>
	
		
	
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>











<script src="/js/main.min.5c74b870c6953931a705f390a49c7e4c0a842ec5c83b24354758dd674343ed0d.js" integrity="sha256-XHS4cMaVOTGnBfOQpJx&#43;TAqELsXIOyQ1R1jdZ0ND7Q0=" crossorigin="anonymous"></script>




  </body>
</html>
